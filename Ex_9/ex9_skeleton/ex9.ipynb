{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d815c0f",
   "metadata": {},
   "source": [
    "# Please install the following python libraries\n",
    "- python3: https://www.python.org/\n",
    "- numpy: https://numpy.org/install/\n",
    "- tqdm: https://github.com/tqdm/tqdm#installation\n",
    "- matplotlib: https://matplotlib.org/stable/users/installing/index.html\n",
    "\n",
    "If you encounter the error: \"IProgress not found. Please update jupyter & ipywidgets\"\n",
    "    \n",
    "Please install the ipywidgets as follows:\n",
    "\n",
    "    with pip, do\n",
    "    - pip install ipywidgets\n",
    "    \n",
    "    with conda, do\n",
    "    - conda install -c conda-forge ipywidgets\n",
    "    \n",
    "Restart your notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eafbfff",
   "metadata": {},
   "source": [
    "## Please install the Pytorch library on your computer before you run this notebook.\n",
    "\n",
    "The installation instructions can be found here. (https://pytorch.org/get-started/locally/)\n",
    "\n",
    "In this exercise, we will use the Pytorch library (https://pytorch.org/) to build and train our deep neural networks. In the deep learning literature, especially in the research community, Pytorch is SUPER popular due to its automatic differentiation and dynamic computational graph (i.e., the graph is automatically generated, which is different from tensorflow where you have to define them beforehand). Briefly spearking, using Pytorch, you only have to build your neural network, define the forward pass, and the loss function. The library will automatically compute the weights and perform the backpropagation for you. For more details about Pytorch, we recommend you check the tutorails on the offical website to learn the basics (https://pytorch.org/tutorials/). \n",
    "\n",
    "Please try to learn the basics as much as you can. If you have any questions, feel free to ask them on Piazza or TA hours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1574259a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from gym import spaces\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace42bed",
   "metadata": {},
   "source": [
    "# Four Rooms environment\n",
    "\n",
    "Note that, to make the environment similar to environments in gym, we change the action space to be [0, 1, 2, 3] using the gym.spaces.Discrete(4) where we can use it to sample a random action. Please read the code below to make sure you understand the changes.\n",
    "\n",
    "Besides, we change the reward function back to the original one. Specifically, the agent receives 1 for reaching the goal but 0 otherwise.\n",
    "\n",
    "Finally, we recommend you to start with the provided hyperparameters to debug.\n",
    "\n",
    "The environment and plotting function are provided. You do have to implement it but feel free to change to \n",
    "adapt them to your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3785f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FourRooms(object):\n",
    "    def __init__(self):\n",
    "        # We define the grid for the Four Rooms domain\n",
    "        self.grid = np.array([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]])\n",
    "\n",
    "        # We define the observation space consisting of all empty cells\n",
    "        # Note: We have to flip the coordinates from (row_idx, column_idx) -> (x, y),\n",
    "        # where x = column_idx, y = 10 - row_idx\n",
    "        self.observation_space = np.argwhere(self.grid == 0.0).tolist()  # Fine all empty cells\n",
    "        self.observation_space = self.arr_coords_to_four_room_coords(self.observation_space)\n",
    "\n",
    "        \"\"\" Note that, we change the action space as follows\n",
    "        \"\"\"\n",
    "        # We define the action space\n",
    "        self.action_movement = {0: np.array([0, 1]),  # up\n",
    "                                1: np.array([0, -1]),  # down\n",
    "                                2: np.array([-1, 0]),  # left\n",
    "                                3: np.array([1, 0])}  # right\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        # We define the start location\n",
    "        self.start_location = [0, 0]\n",
    "\n",
    "        # We define the goal location\n",
    "        self.goal_location = [10, 10]\n",
    "\n",
    "        # We find all wall cells\n",
    "        self.walls = np.argwhere(self.grid == 1.0).tolist()  # find all wall cells\n",
    "        self.walls = self.arr_coords_to_four_room_coords(self.walls)  # convert to Four Rooms coordinates\n",
    "\n",
    "        # This is an episodic task, we define a timeout: maximal time steps = 459\n",
    "        self.max_time_steps = 459\n",
    "\n",
    "        # We define other useful variables\n",
    "        self.agent_location = None  # track the agent's location in one episode.\n",
    "        self.action = None  # track the agent's action\n",
    "        self.t = 0  # track the current time step in one episode\n",
    "\n",
    "    @staticmethod\n",
    "    def arr_coords_to_four_room_coords(arr_coords_list):\n",
    "        \"\"\"\n",
    "        Function converts the array coordinates to the Four Rooms coordinates (i.e, The origin locates at bottom left).\n",
    "        E.g., The coordinates (0, 0) in the numpy array is mapped to (0, 10) in the Four Rooms coordinates.\n",
    "        Args:\n",
    "            arr_coords_list (list): a list variable consists of tuples of locations in the numpy array\n",
    "\n",
    "        Return:\n",
    "            four_room_coords_list (list): a list variable consists of tuples of converted locations in the\n",
    "                                          Four Rooms environment.\n",
    "        \"\"\"\n",
    "        # Note: We have to flip the coordinates from (row_idx, column_idx) -> (x, y),\n",
    "        # where x = column_idx, y = 10 - row_idx\n",
    "        four_room_coords_list = [(column_idx, 10 - row_idx) for (row_idx, column_idx) in arr_coords_list]\n",
    "        return four_room_coords_list\n",
    "\n",
    "    def reset(self):\n",
    "        # We reset the agent's location to the start location\n",
    "        self.agent_location = self.start_location\n",
    "\n",
    "        # We reset the timeout tracker to be 0\n",
    "        self.t = 0\n",
    "\n",
    "        # We set the information\n",
    "        info = {}\n",
    "        return self.agent_location, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            action (int): a int variable (i.e., 0 for \"up\"). See the self.action_movement above for more details\n",
    "        \"\"\"\n",
    "        # With probability 0.8, the agent takes the correct direction.\n",
    "        # With probability 0.2, the agent takes one of the two perpendicular actions.\n",
    "        # For example, if the correct action is \"LEFT\", then\n",
    "        #     - With probability 0.8, the agent takes action \"LEFT\";\n",
    "        #     - With probability 0.1, the agent takes action \"UP\";\n",
    "        #     - With probability 0.1, the agent takes action \"DOWN\".\n",
    "        if np.random.uniform() < 0.2:\n",
    "            if action == 2 or action == 3:\n",
    "                action = np.random.choice([0, 1], 1)[0]\n",
    "            else:\n",
    "                action = np.random.choice([2, 3], 1)[0]\n",
    "\n",
    "        # Convert the agent's location to array\n",
    "        loc_arr = np.array(self.agent_location)\n",
    "\n",
    "        # Convert the action name to movement array\n",
    "        act_arr = self.action_movement[action]\n",
    "\n",
    "        # Compute the agent's next location\n",
    "        next_agent_location = np.clip(loc_arr + act_arr,\n",
    "                                      a_min=np.array([0, 0]),\n",
    "                                      a_max=np.array([10, 10])).tolist()\n",
    "\n",
    "        # Check if the agent crashes into walls, it stays at the current location.\n",
    "        if tuple(next_agent_location) in self.walls:\n",
    "            next_agent_location = self.agent_location\n",
    "\n",
    "        \"\"\" Note that, we change the reward back to the \n",
    "            original one. \n",
    "            1 for reaching the goal\n",
    "            0 otherwise\n",
    "        \"\"\"\n",
    "        # Compute the reward\n",
    "        reward = 1.0 if next_agent_location == self.goal_location else 0.0\n",
    "\n",
    "        # Check the termination\n",
    "        # If the agent reaches the goal, reward = 1, done = True\n",
    "        # If the time steps reaches the maximal number, reward = 0, done = True.\n",
    "        if reward == 1.0 or self.t == self.max_time_steps:\n",
    "            terminated = True\n",
    "        else:\n",
    "            terminated = False\n",
    "\n",
    "        # Update the agent's location, action and time step trackers\n",
    "        self.agent_location = next_agent_location\n",
    "        self.action = action\n",
    "        self.t += 1\n",
    "\n",
    "        return next_agent_location, reward, terminated, False, {}\n",
    "\n",
    "    def render(self):\n",
    "        # plot the agent and the goal\n",
    "        # empty cell = 0\n",
    "        # wall cell = 1\n",
    "        # agent cell = 2\n",
    "        # goal cell = 3\n",
    "        plot_arr = self.grid.copy()\n",
    "        plot_arr[10 - self.agent_location[1], self.agent_location[0]] = 2\n",
    "        plot_arr[10 - self.goal_location[1], self.goal_location[0]] = 3\n",
    "        plt.clf()\n",
    "        plt.title(f\"state={self.agent_location}, act={self.action_movement[self.action]}\")\n",
    "        plt.imshow(plot_arr)\n",
    "        plt.show(block=False)\n",
    "        plt.pause(0.1)\n",
    "\n",
    "    @staticmethod\n",
    "    def test():\n",
    "        my_env = FourRooms()\n",
    "        state, _ = my_env.reset()\n",
    "\n",
    "        for _ in range(100):\n",
    "            action = np.random.choice(list(my_env.action_space.keys()), 1)[0]\n",
    "\n",
    "            next_state, reward, done, _, _ = my_env.step(action)\n",
    "            my_env.render()\n",
    "\n",
    "            if done:\n",
    "                state, _ = my_env.reset()\n",
    "            else:\n",
    "                state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475b6605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(arr_list, legend_list, color_list, ylabel, fig_title):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        arr_list (list): list of results arrays to plot\n",
    "        legend_list (list): list of legends corresponding to each result array\n",
    "        color_list (list): list of color corresponding to each result array\n",
    "        ylabel (string): label of the Y axis\n",
    "\n",
    "        Note that, make sure the elements in the arr_list, legend_list and color_list are associated with each other correctly.\n",
    "        Do not forget to change the ylabel for different plots.\n",
    "    \"\"\"\n",
    "    # set the figure type\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # PLEASE NOTE: Change the labels for different plots\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xlabel(\"Time Steps\")\n",
    "\n",
    "    # ploth results\n",
    "    h_list = []\n",
    "    for arr, legend, color in zip(arr_list, legend_list, color_list):\n",
    "        # compute the standard error\n",
    "        arr_err = arr.std(axis=0) / np.sqrt(arr.shape[0])\n",
    "        # plot the mean\n",
    "        h, = ax.plot(range(arr.shape[1]), arr.mean(axis=0), color=color, label=legend)\n",
    "        # plot the confidence band\n",
    "        arr_err *= 1.96\n",
    "        ax.fill_between(range(arr.shape[1]), arr.mean(axis=0) - arr_err, arr.mean(axis=0) + arr_err, alpha=0.3,\n",
    "                        color=color)\n",
    "        # save the plot handle\n",
    "        h_list.append(h)\n",
    "\n",
    "    # plot legends\n",
    "    ax.set_title(f\"{fig_title}\")\n",
    "    ax.legend(handles=h_list)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300440a8",
   "metadata": {},
   "source": [
    "# Q3 : Implement the REINFORCE algorithm on page 328\n",
    "\n",
    "Here, we will implement the REINFORCE algorithm on page 328 using the neural networks. \n",
    "\n",
    "Same as the ex8, we still use the Pytorch library to build the neural networks and train them.\n",
    "\n",
    "Please read the pseudocode on page 328 carefully and complete the implementation below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78769dd7",
   "metadata": {},
   "source": [
    "## Model the policy using a fully connected neural network\n",
    "\n",
    "Here, you are asked to implement a neural network to approximate the stochastic policy \\pi(a|s). In particular, the architecture of the neural network is as follows:\n",
    "\n",
    "Layer 1: Linear, input size 3, output size 128\n",
    "\n",
    "Activation 1: relu\n",
    "\n",
    "Layer 2: Linear, input size 128, output size 4\n",
    "\n",
    "Activation 2: softmax\n",
    "\n",
    "Note that, instead of using the original state = [x, y], we represent each state as [x / 10, y / 10, 1]. In other words, we use a simple normalized feature.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae35b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PolicyNet, self).__init__()\n",
    "\n",
    "        \"\"\" CODE HERE:\n",
    "                Implement the neural network here\n",
    "        \"\"\"\n",
    "        self.network = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" CODE HERE:\n",
    "                Implement the forward propagation\n",
    "        \"\"\"\n",
    "        y = None\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01422cc1",
   "metadata": {},
   "source": [
    "## Implement the REINFORCE agent as a class\n",
    "\n",
    "We implement the REINFORCE agent as a class. Specifically, we model the policy as a fully connected neural networks. \n",
    "\n",
    "Besides, we implement a function to derive an action from the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f083e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCEAgent(object):\n",
    "    def __init__(self):\n",
    "        # create the policy network\n",
    "        self.policy_net = PolicyNet()\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\" Function to derive an action given a state\n",
    "            Args:\n",
    "                state (list): [x/10, y/10, 1]\n",
    "                \n",
    "            Returns:\n",
    "                action index (int), log_prob (ln(\\pi(action|state)))\n",
    "        \"\"\"\n",
    "        state_tensor = torch.tensor(state).float().view(1, -1)\n",
    "        probs = self.policy_net(state_tensor)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        log_prob = m.log_prob(action)\n",
    "        return action.item(), log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86734f10",
   "metadata": {},
   "source": [
    "## Implement the REINFORCE algorithm as a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7d90af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_average(data, window_size):\n",
    "    \"\"\"Smoothen the 1-d data array using a rollin average.\n",
    "\n",
    "    Args:\n",
    "        data: 1-d numpy.array\n",
    "        window_size: size of the smoothing window\n",
    "\n",
    "    Returns:\n",
    "        smooth_data: a 1-d numpy.array with the same size as data\n",
    "    \"\"\"\n",
    "    assert data.ndim == 1\n",
    "    kernel = np.ones(window_size)\n",
    "    smooth_data = np.convolve(data, kernel) / np.convolve(\n",
    "        np.ones_like(data), kernel\n",
    "    )\n",
    "    return smooth_data[: -window_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d444c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCEAgentTrainer(object):\n",
    "    def __init__(self, agent, env, params):\n",
    "        # Agent object\n",
    "        self.agent = agent\n",
    "        \n",
    "        # Environment object\n",
    "        self.env = env\n",
    "        \n",
    "        # Training parameters\n",
    "        self.params = params\n",
    "\n",
    "        # Lists to store the log probabilities and rewards for one episode\n",
    "        self.saved_log_probs = []\n",
    "        self.saved_rewards = []\n",
    "\n",
    "        # Gamma\n",
    "        self.gamma = params['gamma']\n",
    "        \n",
    "        # Small value for returns normalization\n",
    "        self.eps = params['epsilon']\n",
    "\n",
    "        # Create the optimizer\n",
    "        \"\"\" CODE HERE:\n",
    "                Implement the Adam optimizer with the learning rate in params\n",
    "        \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_state_feature(state):\n",
    "        return [state[0] / 10, state[1] / 10, 1]\n",
    "\n",
    "    def update_agent_policy_network(self):\n",
    "        # We define a list to store the policy loss for each time step\n",
    "        policy_loss = []\n",
    "        \n",
    "        # We define a special list to store the return for each time step\n",
    "        returns = deque()\n",
    "\n",
    "        \"\"\" CODE HERE:\n",
    "                Compute the return for each time step\n",
    "                \n",
    "                Hint: We usually compute the return from the end. Remember to append it \n",
    "                      correctly. You can use \"returns.appendleft(G)\"\n",
    "        \"\"\"\n",
    "        # compute returns for every time step\n",
    "\n",
    "\n",
    "        # normalize the returns: for stablize the training\n",
    "        returns = torch.tensor(returns)\n",
    "        norm_returns = (returns - returns.mean()) / (returns.std() + self.eps)\n",
    "\n",
    "        \"\"\" CODE HERE:\n",
    "                Now, we have the return for each time step and log probability for each time step.\n",
    "                Then, we can compute the loss for each time step using the equation of updating \\theta\n",
    "                in the pseudocode. \n",
    "        \"\"\"\n",
    "        for log_prob, r in zip(self.saved_log_probs, norm_returns):\n",
    "            # compute the loss for each time step\n",
    "\n",
    "        # We sum all the policy loss\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        \n",
    "        \"\"\" CODE HERE:\n",
    "                Implement the one-step backpropagation\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        # after the backpropagation, clear the data to get ready to store the new episode data.\n",
    "        del self.saved_log_probs[:]\n",
    "        del self.saved_rewards[:]\n",
    "\n",
    "        return returns[0].item(), policy_loss.item()\n",
    "\n",
    "    def rollout(self):\n",
    "        \"\"\" Function to collect one episode from the environment\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\" CODE HERE:\n",
    "                Implement the code to collect one episode. \n",
    "                \n",
    "                Specifically, we only collect the rewards and corresponding log probability, which\n",
    "                should be stored in \"self.saved_rewards\" and \"self.saved_log_probs\", respectively. \n",
    "                \n",
    "                This is because we only need the return_t and log_probability to update the weights\n",
    "                of the policy.\n",
    "      \n",
    "        \"\"\"\n",
    "        # reset the env\n",
    "\n",
    "        # start rollout\n",
    "            # render the action and log probability given a state (use the feature of the state)\n",
    "            \n",
    "            # save the log probability to \"self.saved_log_probs\"\n",
    "\n",
    "            # render the next state, reward\n",
    "            \n",
    "            # save the reward to \"self.saved_rewards\"\n",
    "\n",
    "            # check termination\n",
    "\n",
    "    def run_train(self):\n",
    "        # number of the training epsiode\n",
    "        episode_num = self.params['episode_num']\n",
    "        \n",
    "        # list to store the returns and losses during the training\n",
    "        train_returns = []\n",
    "        train_losses = []\n",
    "        \n",
    "        # start the training\n",
    "        ep_bar = tqdm.trange(episode_num)\n",
    "        for ep in ep_bar:\n",
    "            \"\"\"CODE HERE\n",
    "                    Implement the REINFORCE algorithm here.\n",
    "            \"\"\"\n",
    "            # collect one episode\n",
    "\n",
    "            # update the policy using the collected episode\n",
    "            G, loss = None, None\n",
    "            \n",
    "            # save the returns and losses\n",
    "            train_returns.append(G)\n",
    "            train_losses.append(loss)\n",
    "            \n",
    "            # add description\n",
    "            ep_bar.set_description(f\"Episode={ep} | Discounted returns = {G} | loss = {loss:.2f}\")\n",
    "            \n",
    "        # we have to smooth the returns for plotting\n",
    "        smoothed_returns = rolling_average(np.array(train_returns), window_size=100)\n",
    "        smoothed_losses = rolling_average(np.array(train_losses), window_size=100)\n",
    "        \n",
    "        return smoothed_returns, smoothed_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b6b9da",
   "metadata": {},
   "source": [
    "## Main function to run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41adf86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    my_env = FourRooms()\n",
    "\n",
    "    train_params = {\n",
    "        'episode_num': 2000,\n",
    "        'learning_rate': 1e-3,\n",
    "        'gamma': 0.99,\n",
    "        'epsilon': 1e-6,\n",
    "\n",
    "        'action_num': 4,\n",
    "        'observation_dim': 3,\n",
    "\n",
    "        'max_time_step_per_episode': 459\n",
    "    }\n",
    "\n",
    "    results_1_returns = []\n",
    "    results_1_losses = []\n",
    "    for _ in range(2):\n",
    "        my_agent = REINFORCEAgent()\n",
    "\n",
    "        my_trainer = REINFORCEAgentTrainer(my_agent, my_env, train_params)\n",
    "        returns, losses = my_trainer.run_train()\n",
    "        \n",
    "        results_1_returns.append(returns)\n",
    "        results_1_losses.append(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88e1612",
   "metadata": {},
   "source": [
    "# Plot the learning curves: discounted returns w.r.t epsiodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cb557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves([np.array(results_1_returns)],\n",
    "            ['REINFORCE'],\n",
    "            ['r'],\n",
    "            'Discounted returns',\n",
    "            \"Discounted Returns Figure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a15a89",
   "metadata": {},
   "source": [
    "# Plot the loss curves: loss w.r.t epsiodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df38b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves([np.array(results_1_losses)], ['REINFORCE'], ['b'], 'Loss', \"Loss Figure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e3701b",
   "metadata": {},
   "source": [
    "# Q3 : Implement the REINFORCE algorithm with baseline on page 330\n",
    "\n",
    "Here, we will implement the REINFORCE algorithm with baseline on page 330 using the neural networks. \n",
    "\n",
    "Same as the ex8, we still use the Pytorch library to build the neural networks and train them.\n",
    "\n",
    "Please read the pseudocode on page 330 carefully and complete the implementation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aa5fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCEBaselineNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(REINFORCEBaselineNet, self).__init__()\n",
    "\n",
    "        \"\"\" CODE HERE:\n",
    "                Implement the critic network here. The architecture should be as follows:\n",
    "                \n",
    "                Layer 1: Linear, input size 3, output size 128\n",
    "\n",
    "                Activation 1: relu\n",
    "\n",
    "                Layer 2: Linear, input size 128, output size 1\n",
    "\n",
    "                Activation 2: identity\n",
    "        \"\"\"\n",
    "        # critic network\n",
    "        self.critic_network = None\n",
    "\n",
    "        \"\"\" CODE HERE:\n",
    "                Implement the critic network here. The architecture should be as follows:\n",
    "                \n",
    "                Layer 1: Linear, input size 3, output size 128\n",
    "\n",
    "                Activation 1: relu\n",
    "\n",
    "                Layer 2: Linear, input size 128, output size 4\n",
    "\n",
    "                Activation 2: softmax\n",
    "        \"\"\"\n",
    "        # actor network\n",
    "        self.actor_network = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # compute the critic\n",
    "        state_value = self.critic_network(x)\n",
    "\n",
    "        # compute the actor\n",
    "        action_probs = F.softmax(self.actor_network(x), dim=1)\n",
    "\n",
    "        return state_value, action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd8c7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCEBaselineAgent(object):\n",
    "    def __init__(self):\n",
    "        # create the actor network and state value network\n",
    "        self.policy_net = REINFORCEBaselineNet()\n",
    "\n",
    "    def get_action(self, state):\n",
    "        # derive an action, log probability, and the state value\n",
    "        state_tensor = torch.tensor(state).float().view(1, -1)\n",
    "        state_value, action_probs = self.policy_net(state_tensor)\n",
    "        m = Categorical(action_probs)\n",
    "        action = m.sample()\n",
    "        log_prob = m.log_prob(action)\n",
    "        return action.item(), log_prob, state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0348f882",
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCEBaselineAgentTrainer(object):\n",
    "    def __init__(self, agent, env, params):\n",
    "        # Agent object\n",
    "        self.agent = agent\n",
    "        \n",
    "        # Environment object\n",
    "        self.env = env\n",
    "        \n",
    "        # Training parameters\n",
    "        self.params = params\n",
    "\n",
    "        # Lists to store the log probabilities, state values, and rewards for one episode\n",
    "        self.saved_log_probs = []\n",
    "        self.saved_state_values = []\n",
    "        self.saved_rewards = []\n",
    "\n",
    "        # Gamma\n",
    "        self.gamma = params['gamma']\n",
    "\n",
    "        # Small value for returns normalization\n",
    "        self.eps = params['epsilon']\n",
    "\n",
    "        # Create the optimizer\n",
    "        \"\"\" CODE HERE:\n",
    "                Implement the Adam optimizer with the learning rate in params\n",
    "        \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_state_feature(state):\n",
    "        return [state[0] / 10, state[1] / 10, 1]\n",
    "\n",
    "    def update_agent_policy_network(self):\n",
    "        # We define a list to store the policy loss for each time step\n",
    "        policy_loss = []\n",
    "        \n",
    "        # We define a list to store the state value for each time step\n",
    "        value_loss = []\n",
    "        \n",
    "        # We define a list to store the return for each time step\n",
    "        returns = deque()\n",
    "\n",
    "        \"\"\" CODE HERE:\n",
    "                Compute the return for each time step\n",
    "                \n",
    "                Hint: We usually compute the return from the end. Remember to append it \n",
    "                      correctly. You can use \"returns.appendleft(G)\"\n",
    "        \"\"\"\n",
    "        # compute returns for every time step\n",
    "\n",
    "        # normalize the returns\n",
    "        returns = torch.tensor(returns)\n",
    "        norm_returns = (returns - returns.mean()) / (returns.std() + self.eps)\n",
    "\n",
    "        \"\"\" CODE HERE:\n",
    "                Now, we have the return, state value, and log probability for each time step.\n",
    "                Then, we can compute the loss for each time step using the equation of updating \\theta\n",
    "                in the pseudocode. \n",
    "        \"\"\"\n",
    "        for log_prob, val, r in zip(self.saved_log_probs, self.saved_state_values, norm_returns):\n",
    "            # compute the policy loss for each time step\n",
    "\n",
    "        # compute the total loss\n",
    "        total_loss = torch.stack(policy_loss).sum() + torch.stack(value_loss).sum()\n",
    "\n",
    "        # optimization\n",
    "        self.optim.zero_grad()\n",
    "        total_loss.backward()\n",
    "        self.optim.step()\n",
    "\n",
    "        del self.saved_log_probs[:]\n",
    "        del self.saved_state_values[:]\n",
    "        del self.saved_rewards[:]\n",
    "\n",
    "        return returns[0].item(), total_loss.item()\n",
    "\n",
    "    def rollout(self):\n",
    "        \"\"\" CODE HERE:\n",
    "                Implement the code to collect one episode. \n",
    "                \n",
    "                Specifically, we only collect the rewards and corresponding log probability, which\n",
    "                should be stored in \"self.saved_rewards\" and \"self.saved_log_probs\", respectively. \n",
    "                \n",
    "                This is because we only need the return_t and log_probability to update the weights\n",
    "                of the policy.\n",
    "      \n",
    "        \"\"\"\n",
    "        # reset the env\n",
    "\n",
    "        # start rollout\n",
    "            # render the action and log probability given a state (use the feature of the state)\n",
    "            \n",
    "            # save the log probability to \"self.saved_log_probs\"\n",
    "            \n",
    "            # save the state value to \"self.saved_state_values\"\n",
    "\n",
    "            # render the next state, reward\n",
    "            \n",
    "            # save the reward to \"self.saved_rewards\"\n",
    "\n",
    "            # check termination\n",
    "\n",
    "    def run_train(self):\n",
    "        # number of the training epsiode\n",
    "        episode_num = self.params['episode_num']\n",
    "        \n",
    "        # list to store the returns and losses during the training\n",
    "        train_returns = []\n",
    "        train_losses = []\n",
    "        \n",
    "        ep_bar = tqdm.trange(episode_num)\n",
    "        for ep in ep_bar:  \n",
    "            \"\"\"CODE HERE\n",
    "                    Implement the REINFORCE algorithm here.\n",
    "            \"\"\"\n",
    "            # collect one episode\n",
    "\n",
    "            # update the policy using the collected episode\n",
    "\n",
    "            # update the policy using the collected episode\n",
    "            \n",
    "            # save the returns and losses\n",
    "            train_returns.append(G)\n",
    "            train_losses.append(loss)\n",
    "\n",
    "            ep_bar.set_description(f\"Episode={ep} | Discounted returns = {G} | loss = {loss:.2f}\")\n",
    "            \n",
    "        # we have to smooth the returns for plotting\n",
    "        smoothed_returns = rolling_average(np.array(train_returns), window_size=100)\n",
    "        smoothed_losses = rolling_average(np.array(train_losses), window_size=100)\n",
    "        \n",
    "        return smoothed_returns, smoothed_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351169da",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    my_env = FourRooms()\n",
    "\n",
    "    train_params = {\n",
    "        'episode_num': 2000,\n",
    "        'learning_rate': 1e-3,\n",
    "        'gamma': 0.99,\n",
    "        'epsilon': 1e-6,\n",
    "\n",
    "        'action_num': 2,\n",
    "        'observation_dim': 4,\n",
    "\n",
    "        'max_time_step_per_episode': 459,\n",
    "\n",
    "        'input_dim': 3,\n",
    "        'hidden_dim': 128,\n",
    "        'hidden_num': 1,\n",
    "        'output_dim': 4\n",
    "    }\n",
    "\n",
    "    results_2_returns = []\n",
    "    results_2_losses = []\n",
    "    for _ in range(2):\n",
    "        my_agent = REINFORCEBaselineAgent()\n",
    "        my_trainer = REINFORCEBaselineAgentTrainer(my_agent, my_env, train_params)\n",
    "        returns, losses = my_trainer.run_train()\n",
    "        \n",
    "        results_2_returns.append(returns)\n",
    "        results_2_losses.append(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f126457f",
   "metadata": {},
   "source": [
    "# Plot the learning curves: discounted returns w.r.t epsiodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2118cfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves([np.array(results_2_returns)], ['REINFORCE-baseline'], ['r'], 'Discounted returns', \"Discounted Returns Figure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667be120",
   "metadata": {},
   "source": [
    "# Plot the loss curves: loss w.r.t epsiodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4995e778",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves([np.array(results_2_losses)], ['REINFORCE-baseline'], ['b'], 'Loss', \"Loss Figure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fa5072",
   "metadata": {},
   "source": [
    "# Q3 - Compare vanilla REINFORCE algorithm v.s. REINFORCE with baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4922ee06",
   "metadata": {},
   "source": [
    "# Compare learning curves : discounted returns w.r.t epsiodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4906214",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves([np.array(results_1_returns), np.array(results_2_returns)], \n",
    "            ['REINFORCE', 'REINFORCE-baseline'], \n",
    "            ['r', 'b'], 'Discounted returns', \"Discounted Returns Figure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8026e491",
   "metadata": {},
   "source": [
    "# Compare loss curves : loss w.r.t epsiodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e0fdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves([np.array(results_1_losses), np.array(results_2_losses)], \n",
    "            ['REINFORCE', 'REINFORCE-baseline'], \n",
    "            ['r', 'b'], 'Loss', \"Loss Figure\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
