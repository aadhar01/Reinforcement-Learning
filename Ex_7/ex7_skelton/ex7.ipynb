{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c8fb627",
   "metadata": {},
   "source": [
    "# Please install the following python libraries\n",
    "- python3: https://www.python.org/\n",
    "- numpy: https://numpy.org/install/\n",
    "- tqdm: https://github.com/tqdm/tqdm#installation\n",
    "- matplotlib: https://matplotlib.org/stable/users/installing/index.html\n",
    "\n",
    "If you encounter the error: \"IProgress not found. Please update jupyter & ipywidgets\"\n",
    "    \n",
    "Please install the ipywidgets as follows:\n",
    "\n",
    "    with pip, do\n",
    "    - pip install ipywidgets\n",
    "    \n",
    "    with conda, do\n",
    "    - conda install -c conda-forge ipywidgets\n",
    "    \n",
    "Restart your notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e69a6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0c2a45",
   "metadata": {},
   "source": [
    "# Q3: Solving Four Rooms using semi-gradient SARSA with state aggregation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adaa389",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Four Rooms Environment Implementation\n",
    "\"\"\"\n",
    "class FourRooms(object):\n",
    "    def __init__(self):\n",
    "        # We define the grid for the Four Rooms domain\n",
    "        self.grid = np.array([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]])\n",
    "\n",
    "        # We define the observation space consisting of all empty cells\n",
    "        # Note: We have to flip the coordinates from (row_idx, column_idx) -> (x, y),\n",
    "        # where x = column_idx, y = 10 - row_idx\n",
    "        self.observation_space = np.argwhere(self.grid == 0.0).tolist()  # Fine all empty cells\n",
    "        self.observation_space = self.arr_coords_to_four_room_coords(self.observation_space)\n",
    "\n",
    "        # We define the action space\n",
    "        self.action_space = {'up': np.array([0, 1]),\n",
    "                             'down': np.array([0, -1]),\n",
    "                             'left': np.array([-1, 0]),\n",
    "                             'right': np.array([1, 0])}\n",
    "        self.action_names = ['up', 'down', 'left', 'right']\n",
    "\n",
    "        # We define the start location\n",
    "        self.start_location = [0, 0]\n",
    "\n",
    "        # We define the goal location\n",
    "        self.goal_location = [10, 10]\n",
    "\n",
    "        # We find all wall cells\n",
    "        self.walls = np.argwhere(self.grid == 1.0).tolist()  # find all wall cells\n",
    "        self.walls = self.arr_coords_to_four_room_coords(self.walls)  # convert to Four Rooms coordinates\n",
    "\n",
    "        # This is an episodic task, we define a timeout: maximal time steps = 459\n",
    "        self.max_time_steps = 459\n",
    "\n",
    "        # We define other useful variables\n",
    "        self.agent_location = None  # track the agent's location in one episode.\n",
    "        self.action = None  # track the agent's action\n",
    "        self.t = 0  # track the current time step in one episode\n",
    "\n",
    "    @staticmethod\n",
    "    def arr_coords_to_four_room_coords(arr_coords_list):\n",
    "        \"\"\"\n",
    "        Function converts the array coordinates to the Four Rooms coordinates (i.e, The origin locates at bottom left).\n",
    "        E.g., The coordinates (0, 0) in the numpy array is mapped to (0, 10) in the Four Rooms coordinates.\n",
    "        Args:\n",
    "            arr_coords_list (list): a list variable consists of tuples of locations in the numpy array\n",
    "\n",
    "        Return:\n",
    "            four_room_coords_list (list): a list variable consists of tuples of converted locations in the\n",
    "                                          Four Rooms environment.\n",
    "        \"\"\"\n",
    "        # Note: We have to flip the coordinates from (row_idx, column_idx) -> (x, y),\n",
    "        # where x = column_idx, y = 10 - row_idx\n",
    "        four_room_coords_list = [(column_idx, 10 - row_idx) for (row_idx, column_idx) in arr_coords_list]\n",
    "        return four_room_coords_list\n",
    "\n",
    "    def reset(self):\n",
    "        # We reset the agent's location to the start location\n",
    "        self.agent_location = self.start_location\n",
    "\n",
    "        # We reset the timeout tracker to be 0\n",
    "        self.t = 0\n",
    "\n",
    "        # We set the information\n",
    "        info = {}\n",
    "        return self.agent_location, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            action (string): a string variable (i.e., \"UP\"). All feasible values are [\"up\", \"down\", \"left\", \"right\"].\n",
    "        \"\"\"\n",
    "        # With probability 0.8, the agent takes the correct direction.\n",
    "        # With probability 0.2, the agent takes one of the two perpendicular actions.\n",
    "        # For example, if the correct action is \"LEFT\", then\n",
    "        #     - With probability 0.8, the agent takes action \"LEFT\";\n",
    "        #     - With probability 0.1, the agent takes action \"UP\";\n",
    "        #     - With probability 0.1, the agent takes action \"DOWN\".\n",
    "        if np.random.uniform() < 0.2:\n",
    "            if action == \"left\" or action == \"right\":\n",
    "                action = np.random.choice([\"up\", \"down\"], 1)[0]\n",
    "            else:\n",
    "                action = np.random.choice([\"right\", \"left\"], 1)[0]\n",
    "\n",
    "        # Convert the agent's location to array\n",
    "        loc_arr = np.array(self.agent_location)\n",
    "\n",
    "        # Convert the action name to movement array\n",
    "        act_arr = self.action_space[action]\n",
    "\n",
    "        # Compute the agent's next location\n",
    "        next_agent_location = np.clip(loc_arr + act_arr,\n",
    "                                      a_min=np.array([0, 0]),\n",
    "                                      a_max=np.array([10, 10])).tolist()\n",
    "\n",
    "        # Check if the agent crashes into walls, it stays at the current location.\n",
    "        if tuple(next_agent_location) in self.walls:\n",
    "            next_agent_location = self.agent_location\n",
    "\n",
    "        # Compute the reward\n",
    "        reward = 1.0 if next_agent_location == self.goal_location else 0.0\n",
    "\n",
    "        # Check the termination\n",
    "        # If the agent reaches the goal, reward = 1, done = True\n",
    "        # If the time steps reaches the maximal number, reward = 0, done = True.\n",
    "        if reward == 1.0 or self.t == self.max_time_steps:\n",
    "            terminated = True\n",
    "        else:\n",
    "            terminated = False\n",
    "\n",
    "        # Update the agent's location, action and time step trackers\n",
    "        self.agent_location = next_agent_location\n",
    "        self.action = action\n",
    "        self.t += 1\n",
    "\n",
    "        return next_agent_location, reward, terminated, False, {}\n",
    "\n",
    "    def render(self):\n",
    "        # plot the agent and the goal\n",
    "        # empty cell = 0\n",
    "        # wall cell = 1\n",
    "        # agent cell = 2\n",
    "        # goal cell = 3\n",
    "        plot_arr = self.grid.copy()\n",
    "        plot_arr[10 - self.agent_location[1], self.agent_location[0]] = 2\n",
    "        plot_arr[10 - self.goal_location[1], self.goal_location[0]] = 3\n",
    "        plt.clf()\n",
    "        plt.title(f\"state={self.agent_location}, act={self.action}\")\n",
    "        plt.imshow(plot_arr)\n",
    "        plt.show(block=False)\n",
    "        plt.pause(0.1)\n",
    "\n",
    "    @staticmethod\n",
    "    def test():\n",
    "        my_env = FourRooms()\n",
    "        state, _ = my_env.reset()\n",
    "\n",
    "        for _ in range(100):\n",
    "            action = np.random.choice(list(my_env.action_space.keys()), 1)[0]\n",
    "\n",
    "            next_state, reward, done, _, _ = my_env.step(action)\n",
    "            my_env.render()\n",
    "\n",
    "            if done:\n",
    "                state, _ = my_env.reset()\n",
    "            else:\n",
    "                state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d4406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(arr_list, legend_list, color_list, ylabel, fig_title):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        arr_list (list): list of results arrays to plot\n",
    "        legend_list (list): list of legends corresponding to each result array\n",
    "        color_list (list): list of color corresponding to each result array\n",
    "        ylabel (string): label of the Y axis\n",
    "\n",
    "        Note that, make sure the elements in the arr_list, legend_list and color_list are associated with each other correctly.\n",
    "        Do not forget to change the ylabel for different plots.\n",
    "    \"\"\"\n",
    "    # set the figure type\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # PLEASE NOTE: Change the labels for different plots\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xlabel(\"Time Steps\")\n",
    "\n",
    "    # ploth results\n",
    "    h_list = []\n",
    "    for arr, legend, color in zip(arr_list, legend_list, color_list):\n",
    "        # compute the standard error\n",
    "        arr_err = arr.std(axis=0) / np.sqrt(arr.shape[0])\n",
    "        # plot the mean\n",
    "        h, = ax.plot(range(arr.shape[1]), arr.mean(axis=0), color=color, label=legend)\n",
    "        # plot the confidence band\n",
    "        arr_err *= 1.96\n",
    "        ax.fill_between(range(arr.shape[1]), arr.mean(axis=0) - arr_err, arr.mean(axis=0) + arr_err, alpha=0.3,\n",
    "                        color=color)\n",
    "        # save the plot handle\n",
    "        h_list.append(h)\n",
    "\n",
    "    # plot legends\n",
    "    ax.set_title(f\"{fig_title}\")\n",
    "    ax.legend(handles=h_list)\n",
    "\n",
    "    # save the figure\n",
    "    plt.savefig(f\"{fig_title}.png\", dpi=200)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1118c4",
   "metadata": {},
   "source": [
    "# Q3 - (a): Implement the semi-gradient SARSA\n",
    "\n",
    "As described in the question, you are asked to implement the semi-gradient SARSA with a very simple state aggregation strategy. That is aggregating both states and actions to itself. Indeed, this will have similar results as applying *tabular* SARSA directly. \n",
    "\n",
    "**Please implement the following state aggregation strategy**\n",
    "\n",
    "- For each state, its aggregated state is itself. E.g. [0, 0] is aggregated to [0, 0] only. \n",
    "- For each action, its aggregated action is also itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53298add",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemiGradientSARSAAgent(object):\n",
    "    def __init__(self, env, info):\n",
    "        \"\"\"\n",
    "        Function to initialize the semi-gradient SARSA agent\n",
    "        Args:\n",
    "            env: the environment that the agent interacts with\n",
    "            info (dict): a dictionary variable contains all necessary parameters.\n",
    "\n",
    "        Note that: In this question, we will implement a simple state aggregation strategies.\n",
    "                   Specifically, we design the following function approximation:\n",
    "                   1. Feature: we use the one-hot encoding to compute the feature for each state-action pair.\n",
    "                               E.g., state = [0, 0] and action = \"Up\" (state-action pair) will correspond to\n",
    "                               a unique one-hot representation $f(s, a) = [0, 0, 0, 1, 0, ..., 0]$.\n",
    "                   2. Weights: we define a weight vector $w$ having the sample shape as the feature vector.\n",
    "                               Specifically, the Q(s, a) can be estimated by Q(s, a) = w^{T} * f(s, a)\n",
    "\n",
    "        Importantly, as described in the question, we only aggregate the states.\n",
    "        \"\"\"\n",
    "        # Store the environment\n",
    "        self.env = env\n",
    "\n",
    "        \"\"\" Learning parameters for semi-gradient SARSA \"\"\"\n",
    "        # Store the number of learning episodes\n",
    "        self.episode_num = info['episode_num']\n",
    "\n",
    "        # Store the update step size alpha\n",
    "        self.alpha = info['alpha']\n",
    "\n",
    "        # Store the discount factor\n",
    "        self.gamma = info['gamma']\n",
    "\n",
    "        # Initialize the epsilon\n",
    "        self.epsilon = info['epsilon']\n",
    "\n",
    "        # Store the other hyerparameters\n",
    "        self.params = info\n",
    "\n",
    "        \"\"\" State aggregation for semi-gradient SARSA \"\"\"\n",
    "        # Compute the total number of state after the aggregation\n",
    "        self.state_space, self.state_num = self.create_state_aggregation()\n",
    "\n",
    "        # Compute the total number of the actions\n",
    "        self.action_num = len(self.env.action_names)\n",
    "\n",
    "        \"\"\" Function approximation for semi-gradient SARSA\"\"\"\n",
    "        # We create a weight with shape |S| * |A|\n",
    "        self.weights_fn = np.zeros((self.state_num * self.action_num))\n",
    "\n",
    "        # We construct a numpy array that contains the one-hot features for all state-action pairs.\n",
    "        # The size is (|S| * |A|) x (|S| * |A|).\n",
    "        # Each i-th row is the one-hot encoding for state-action pair with index i.\n",
    "        self.feature_arr = np.eye(self.state_num * self.action_num)\n",
    "\n",
    "    def create_state_aggregation(self):\n",
    "        \"\"\"\n",
    "        Function that returns the aggregated state space and the number of the aggregated states.\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"CODE HERE: your state aggregation strategy.\n",
    "           \n",
    "           You have to return:\n",
    "            1. the aggregated state space (Any data structure that you find it easier to render the index of the \n",
    "               aggregated state.)\n",
    "            2. the number of the aggregated states (int)\n",
    "        \"\"\"\n",
    "        \n",
    "        # compute the aggregated state space based on the state aggregation strategy\n",
    "        aggregated_state_space = None\n",
    "\n",
    "        # compute the number of the state in the aggregated state space\n",
    "        aggregate_state_num = None\n",
    "\n",
    "        return aggregated_state_space, aggregate_state_num\n",
    "\n",
    "    def _aggregate_state_idx(self, state):\n",
    "        \"\"\"\n",
    "        Function returns the index of aggregated state given an original state\n",
    "\n",
    "        Args:\n",
    "            state (list): original state\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"CODE HERE: based on your state aggregation, return the index of the aggregated state given an original\n",
    "           state. \n",
    "           \n",
    "           You have to return:\n",
    "           1. index (int) of the aggregated state given the original state\n",
    "        \"\"\"\n",
    "        state_idx = None\n",
    "        return state_idx\n",
    "\n",
    "    def _aggregate_action_idx(self, action):\n",
    "        \"\"\"\n",
    "        Function returns the index of aggregated action.\n",
    "        Args:\n",
    "            action (string): name of the action\n",
    "\n",
    "        To be simple, here, one action only aggregates to itself\n",
    "        \"\"\"\n",
    "        return self.env.action_names.index(action)\n",
    "\n",
    "    def _get_state_action_feature(self, state, action):\n",
    "        \"\"\"\n",
    "        Function that returns the one-hot feature given a state-action pair.\n",
    "\n",
    "        Args:\n",
    "            state (list): original state\n",
    "            action (string): name of the action\n",
    "        \"\"\"\n",
    "        # Get the unique index of the aggregated state\n",
    "        state_index = self._aggregate_state_idx(state)\n",
    "        # Get the unique index of the aggregated action\n",
    "        action_index = self._aggregate_action_idx(action)\n",
    "        # Compute the state(aggregated)-action index\n",
    "        state_action_index = self.state_num * action_index + state_index\n",
    "        # Get the one-hot feature of the state\n",
    "        return self.feature_arr[state_action_index]\n",
    "\n",
    "    def function_approximation(self, state, action):\n",
    "        \"\"\"\n",
    "        Function that computes the Q value given a state-action pair using linear function approximation.\n",
    "        Args:\n",
    "            state (list): original state\n",
    "            action (string): name of the action\n",
    "        \"\"\"\n",
    "        state_action_feature = self._get_state_action_feature(state, action)\n",
    "        return np.matmul(state_action_feature.T, self.weights_fn)\n",
    "\n",
    "    def render_q_value(self, state, action):\n",
    "        \"\"\"\n",
    "        Function that returns the Q value given a state-action pair\n",
    "\n",
    "        Args:\n",
    "            state (list): original state\n",
    "            action (string): name of the action\n",
    "        \"\"\"\n",
    "        return self.function_approximation(state, action)\n",
    "\n",
    "    def epsilon_greedy_policy(self, state):\n",
    "        \"\"\"\n",
    "        Function implements the epsilon-greedy policy\n",
    "        Args:\n",
    "            state (list): original state\n",
    "        \"\"\"\n",
    "        \"\"\"CODE HERE:\n",
    "           implement the epsilon-greedy policy using function approximation. Break ties if happens \n",
    "           \n",
    "           You should return:\n",
    "           1. name of the action (string)\n",
    "        \"\"\"\n",
    "        action = None\n",
    "        return action\n",
    "\n",
    "    def update_weights(self, s, a, r, s_prime, a_prime):\n",
    "        \"\"\"\n",
    "        Function that updates the weights using semi-gradients\n",
    "\n",
    "        Args:\n",
    "            s (list): original state\n",
    "            a (string): action name\n",
    "            r (float): reward\n",
    "            s_prime (list): original next state\n",
    "            a_prime (string): next action name\n",
    "        \"\"\"\n",
    "        \"\"\" CODE HERE:\n",
    "            implement the update of the semi-gradient SARSA\n",
    "            \n",
    "            You should update \"self.weights_fn\"\n",
    "        \"\"\"\n",
    "        # check if s_prime is the termination state\n",
    "        \n",
    "        # update the weights\n",
    "\n",
    "    def run(self):\n",
    "        # Save the discounted return for each episode\n",
    "        discounted_returns = []\n",
    "\n",
    "        # Semi-gradient SARSA starts\n",
    "        for ep in tqdm.trange(self.episode_num):\n",
    "            \"\"\"CODE HERE:\n",
    "               Implement the pseudocode of the Semi-gradient SARSA\n",
    "            \"\"\"\n",
    "            # Reset the agent to initial STATE at the beginning of every episode\n",
    "\n",
    "            # Render an ACTION based on the initial STATE\n",
    "\n",
    "            # Store rewards to compute return G for the current episode.\n",
    "            reward_list = []\n",
    "            \n",
    "            # Loop the episode\n",
    "            for t in range(self.env.max_time_steps):\n",
    "                # Take the ACTION and observe REWARD and NEXT STATE\n",
    "\n",
    "                # Given the NEXT STATE, choose the NEXT ACTION\n",
    "\n",
    "                # Update the weights of the function using semi-gradient SARSA\n",
    "                # Using STATE, ACTION, REWARD, NEXT STATE, NEXT ACTION\n",
    "                \n",
    "                \"\"\"DO NOT CHANGE BELOW\"\"\"\n",
    "                # Save the reward for plotting\n",
    "                reward_list.append(reward)\n",
    "\n",
    "                # Reset the environment\n",
    "                if done:\n",
    "                    break\n",
    "                else:\n",
    "                    state = next_state\n",
    "                    action = next_action\n",
    "\n",
    "            # compute the discounted return for the current episode\n",
    "            G = 0\n",
    "            for reward in reversed(reward_list):\n",
    "                G = reward + self.gamma * G\n",
    "            discounted_returns.append(G)\n",
    "\n",
    "        return discounted_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea88e472",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # set the random seed\n",
    "    np.random.seed(1234)\n",
    "    random.seed(1234)\n",
    "\n",
    "    # set hyper-parameters\n",
    "    params = {\n",
    "        \"episode_num\": 100,\n",
    "        \"alpha\": 0.1,\n",
    "        \"gamma\": 0.99,\n",
    "        \"epsilon\": 0.1\n",
    "    }\n",
    "\n",
    "    # set running trials. You can try run_trial = 5 for debugging\n",
    "    run_trial = 10\n",
    "\n",
    "    # run multiple trials\n",
    "    results = []\n",
    "    for _ in range(run_trial):\n",
    "        # create the environment\n",
    "        my_env = FourRooms()\n",
    "\n",
    "        # run semi-gradient SARSA\n",
    "        my_agent = SemiGradientSARSAAgent(my_env, params)\n",
    "        res = my_agent.run()\n",
    "\n",
    "        # save result for each running trial\n",
    "        results.append(np.array(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008c2611",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves([np.array(results)], [\"semi-gradient SARSA\"], [\"b\"],\n",
    "            \"Averaged discounted return\", \"Q3 - (a): semi-gradient SARSA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3a745f",
   "metadata": {},
   "source": [
    "# Q3 - (b) [5180]: Implement the semi-gradient SARSA with Tile-based/Room-based aggregation.\n",
    "\n",
    "As described in the question, you are asked to implement the semi-gradient SARSA with **Tile-based/Room-based** state aggregation strategy. That is grouping the nearby states in a n x n (i.e., n = 2) tile as one aggregated state and grouping states in one room as one aggregated state. \n",
    "\n",
    "**Plot**: Plot the learning curves of tile size n = 2 and Room-based aggregation in the same plot. You can use the plot function above to generate the plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7949861f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Implement the Tile-based Agent here. We inherit it from the \"SemiGradientSARSAAgent\" above\n",
    "\"\"\"\n",
    "class TileAgent(SemiGradientSARSAAgent):\n",
    "    def __init__(self, env, info):\n",
    "        \"\"\"\n",
    "        Function to initialize the semi-gradient SARSA agent\n",
    "        Args:\n",
    "            env: the environment that the agent interacts with\n",
    "            info (dict): a dictionary variable contains all necessary parameters.\n",
    "\n",
    "        Note that: In this question, we will implement a simple state aggregation strategies.\n",
    "                   Specifically, we design the following function approximation:\n",
    "                   1. Feature: we use the one-hot encoding to compute the feature for each state-action pair.\n",
    "                               E.g., state = [0, 0] and action = \"Up\" (state-action pair) will correspond to\n",
    "                               a unique one-hot representation $f(s, a) = [0, 0, 0, 1, 0, ..., 0]$.\n",
    "                   2. Weights: we define a weight vector $w$ having the sample shape as the feature vector.\n",
    "                               Specifically, the Q(s, a) can be estimated by Q(s, a) = w^{T} * f(s, a)\n",
    "\n",
    "        Importantly, as described in the question, we only aggregate the states.\n",
    "        \"\"\"\n",
    "        super().__init__(env, info)\n",
    "        \n",
    "        \"\"\" State aggregation for semi-gradient SARSA \"\"\"\n",
    "        # Compute the total number of state after the aggregation\n",
    "        self.state_space, self.state_num = self.create_state_aggregation()\n",
    "\n",
    "        # Compute the total number of the actions\n",
    "        self.action_num = len(self.env.action_names)\n",
    "\n",
    "        \"\"\" Function approximation for semi-gradient SARSA\"\"\"\n",
    "        # We create a weight with shape |S| * |A|\n",
    "        self.weights_fn = np.zeros((self.state_num * self.action_num))\n",
    "\n",
    "        # We construct a numpy array that contains the one-hot features for all state-action pairs.\n",
    "        # The size is (|S| * |A|) x (|S| * |A|).\n",
    "        # Each i-th row is the one-hot encoding for state-action pair with index i.\n",
    "        self.feature_arr = np.eye(self.state_num * self.action_num)\n",
    "\n",
    "    def create_state_aggregation(self):\n",
    "        \"\"\"\n",
    "        Function that returns the aggregated state space and the number of the aggregated states.\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"CODE HERE: Implement the Tile-based state aggregation here.\n",
    "           Hint: you can manually discretize the original states using the Tile-based method.\n",
    "           For example, you can copy the grid from the Four Rooms environment\n",
    "           and manually aggregate the states (value = 0) in the grid. \n",
    "           \n",
    "           You have to return:\n",
    "            1. the aggregated state space (Any data structure that you find it easier to render the index of the \n",
    "               aggregated state.)\n",
    "            2. the number of the aggregated states (int)\n",
    "        \"\"\"\n",
    "        \n",
    "        # define the aggregated state space using Tile-based method (2x2)\n",
    "        aggregated_state_space = None\n",
    "\n",
    "        # aggregate the state space\n",
    "        aggregate_state_num = None\n",
    "\n",
    "        return aggregated_state_space, aggregate_state_num\n",
    "\n",
    "    def _aggregate_state_idx(self, state):\n",
    "        \"\"\"\n",
    "        Function returns the index of aggregated state given an original state\n",
    "\n",
    "        Args:\n",
    "            state (list): original state\n",
    "        \"\"\"\n",
    "        \"\"\"CODE HERE: based on your state aggregation, return the index of the aggregated state given an original\n",
    "           state. \n",
    "           \n",
    "           You have to return:\n",
    "           1. index (int) of the aggregated state given the original state\n",
    "        \"\"\"\n",
    "        \n",
    "        # render the index of the aggregated state\n",
    "        state_idx = None\n",
    "        return state_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fcaec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Implement the Room-based Agent here. We inherit it from the \"SemiGradientSARSAAgent\" above\n",
    "\"\"\"\n",
    "class RoomAgent(SemiGradientSARSAAgent):\n",
    "    def __init__(self, env, info):\n",
    "        \"\"\"\n",
    "        Function to initialize the semi-gradient SARSA agent\n",
    "        Args:\n",
    "            env: the environment that the agent interacts with\n",
    "            info (dict): a dictionary variable contains all necessary parameters.\n",
    "\n",
    "        Note that: In this question, we will implement a simple state aggregation strategies.\n",
    "                   Specifically, we design the following function approximation:\n",
    "                   1. Feature: we use the one-hot encoding to compute the feature for each state-action pair.\n",
    "                               E.g., state = [0, 0] and action = \"Up\" (state-action pair) will correspond to\n",
    "                               a unique one-hot representation $f(s, a) = [0, 0, 0, 1, 0, ..., 0]$.\n",
    "                   2. Weights: we define a weight vector $w$ having the sample shape as the feature vector.\n",
    "                               Specifically, the Q(s, a) can be estimated by Q(s, a) = w^{T} * f(s, a)\n",
    "\n",
    "        Importantly, as described in the question, we only aggregate the states.\n",
    "        \"\"\"\n",
    "        super().__init__(env, info)\n",
    "        \n",
    "        \"\"\" State aggregation for semi-gradient SARSA \"\"\"\n",
    "        # Compute the total number of state after the aggregation\n",
    "        self.state_space, self.state_num = self.create_state_aggregation()\n",
    "\n",
    "        # Compute the total number of the actions\n",
    "        self.action_num = len(self.env.action_names)\n",
    "\n",
    "        \"\"\" Function approximation for semi-gradient SARSA\"\"\"\n",
    "        # We create a weight with shape |S| * |A|\n",
    "        self.weights_fn = np.zeros((self.state_num * self.action_num))\n",
    "\n",
    "        # We construct a numpy array that contains the one-hot features for all state-action pairs.\n",
    "        # The size is (|S| * |A|) x (|S| * |A|).\n",
    "        # Each i-th row is the one-hot encoding for state-action pair with index i.\n",
    "        self.feature_arr = np.eye(self.state_num * self.action_num)\n",
    "\n",
    "    def create_state_aggregation(self):\n",
    "        \"\"\"\n",
    "        Function that returns the aggregated state space and the number of the aggregated states.\n",
    "        \"\"\"\n",
    "        \"\"\"CODE HERE: your state aggregation strategy. Hint: you can start with a simple state aggregation\n",
    "           that just aggregate each state to itself. In other words, the aggregated state space is just\n",
    "           the original state space.\n",
    "           \n",
    "           You have to return:\n",
    "            1. the aggregated state space (Any data structure that you find it easier to render the index of the \n",
    "               aggregated state.)\n",
    "            2. the number of the aggregated states (int)\n",
    "        \"\"\"\n",
    "\n",
    "        aggregated_state_space = None\n",
    "\n",
    "        # aggregate the state space\n",
    "        aggregate_state_num = None\n",
    "\n",
    "        return aggregated_state_space, aggregate_state_num\n",
    "\n",
    "    def _aggregate_state_idx(self, state):\n",
    "        \"\"\"\n",
    "        Function returns the index of aggregated state given an original state\n",
    "\n",
    "        Args:\n",
    "            state (list): original state\n",
    "        \"\"\"\n",
    "        \"\"\"CODE HERE: based on your state aggregation, return the index of the aggregated state given an original\n",
    "           state. \n",
    "           \n",
    "           You have to return:\n",
    "           1. index (int) of the aggregated state given the original state\n",
    "        \"\"\"\n",
    "        state_idx = None\n",
    "        return state_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ad0474",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # set the random seed\n",
    "    np.random.seed(1234)\n",
    "    random.seed(1234)\n",
    "\n",
    "    # set hyper-parameters\n",
    "    params = {\n",
    "        \"episode_num\": 100,\n",
    "        \"alpha\": 0.1,\n",
    "        \"gamma\": 0.99,\n",
    "        \"epsilon\": 0.1\n",
    "    }\n",
    "\n",
    "    # set running trials; You can trial run_trial = 5 to debug\n",
    "    run_trial = 10\n",
    "\n",
    "    # run experiment for the Tile-based method\n",
    "    results_tile = []\n",
    "    for _ in range(run_trial):        \n",
    "        # create the environment\n",
    "        my_env = FourRooms()\n",
    "\n",
    "        # run semi-gradient SARSA with Tile-based method with tile size n = 2\n",
    "        my_agent = TileAgent(my_env, params)\n",
    "        res = my_agent.run()\n",
    "\n",
    "        # save result for each running trial\n",
    "        results_tile.append(np.array(res))\n",
    "        \n",
    "    # run experiment for the Room-based method\n",
    "    results_room = []\n",
    "    for _ in range(run_trial):        \n",
    "        # create the environment\n",
    "        my_env = FourRooms()\n",
    "\n",
    "        # run semi-gradient SARSA with Room-based method\n",
    "        my_agent = RoomAgent(my_env, params)\n",
    "        res = my_agent.run()\n",
    "\n",
    "        # save result for each running trial\n",
    "        results_room.append(np.array(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205517c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "plot_curves([np.array(results), np.array(results_tile), np.array(results_room)],\n",
    "            [\"State-aggregation: identical\", \"State-aggregation: Tile = 2x2\", \"State aggregation: Room-based\"],\n",
    "            [\"b\", \"r\", \"g\"],\n",
    "            \"Averaged discounted return\", \"Q3 - (b): Comparison between three state aggregation strategies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8161caab",
   "metadata": {},
   "source": [
    "# Q3 - (d):  Adapt your implementation of semi-gradient one-step SARSA for linear function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eefc427",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implement your code here.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475f601f",
   "metadata": {},
   "source": [
    "# Q3 - (e) [5180]: Implement the following two features, and plot the learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4008d499",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implement your code here.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2f778085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"The distance matrix is here\"\"\"\n",
    "\n",
    "distance_matrix = np.array([[14, 13, 12, 11, 10, -1,  4,  3,  2,  1,  0],\n",
    "                            [13, 12, 11, 10,  9, -1,  5,  4,  3,  2,  1],\n",
    "                            [12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2],\n",
    "                            [13, 12, 11, 10,  9, -1,  7,  6,  5,  4,  3],\n",
    "                            [14, 13, 12, 11, 10, -1,  8,  7,  6,  5,  4],\n",
    "                            [-1, 14, -1, -1, -1, -1,  9,  8,  7,  6,  5],\n",
    "                            [16, 15, 16, 17, 18, -1, -1, -1,  8, -1, -1],\n",
    "                            [17, 16, 17, 18, 17, -1, 11, 10,  9, 10, 11],\n",
    "                            [18, 17, 18, 17, 16, -1, 12, 11, 10, 11, 12],\n",
    "                            [19, 18, 17, 16, 15, 14, 13, 12, 11, 12, 13],\n",
    "                            [20, 19, 18, 17, 16, -1, 14, 13, 12, 13, 14]])\n",
    "plt.imshow(distance_matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339ab5cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
