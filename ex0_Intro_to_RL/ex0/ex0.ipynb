{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please install the following python libraries\n",
    "- python3: https://www.python.org/\n",
    "- numpy: https://numpy.org/install/\n",
    "- tqdm: https://github.com/tqdm/tqdm#installation\n",
    "- matplotlib: https://matplotlib.org/stable/users/installing/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tqdm.notebook as tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Complete the Implementation of the Four Rooms environment \n",
    "\n",
    "- The FourRooms is implemented as a python class. We explain the attributes and methods as follows\n",
    "    - **init** function: Define all the attributes of the Four Rooms environment. For example, the state space, the action space, the start state, the goal state and so on.\n",
    "    - **reset** function: Resets the agent to the start state (0, 0)\n",
    "    - **step** function: Takes the current state and one action, returns the next state and a reward\n",
    "   \n",
    "- Please complete the implementation in the step function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOUR ROOM ENVIRONMENT\n",
    "class FourRooms(object):\n",
    "    def __init__(self):\n",
    "        # define the four room as a 2-D array for easy state space reference and visualization\n",
    "        # 0 represents an empty cell; 1 represents a wall cell\n",
    "        self.four_room_space = np.array([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                                         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                                         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                                         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                                         [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "                                         [0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1],\n",
    "                                         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                                         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                                         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                                         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]])\n",
    "        \n",
    "        # find the positions for all empty cells\n",
    "        # not that: the origin for a 2-D numpy array is located at top-left while the origin for the FourRooms is at\n",
    "        # the bottom-left. The following codes performs the re-projection.\n",
    "        empty_cells = np.where(self.four_room_space == 0.0)\n",
    "        self.state_space = [[col, 10 - row] for row, col in zip(empty_cells[0], empty_cells[1])]\n",
    "\n",
    "        # define the action space\n",
    "        self.action_space = {'LEFT': np.array([-1, 0]),\n",
    "                             'RIGHT': np.array([1, 0]),\n",
    "                             'DOWN': np.array([0, -1]),\n",
    "                             'UP': np.array([0, 1])}\n",
    "        \n",
    "        # define the start state\n",
    "        self.start_state = [0, 0]\n",
    "        \n",
    "        # define the goal state\n",
    "        self.goal_state = [10, 10]\n",
    "        \n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the agent's state to the start state [0, 0]\n",
    "        Return both the start state and reward\n",
    "        \"\"\"\n",
    "        state = self.start_state  # reset the agent to [0, 0]\n",
    "        reward = 0  # reward is 0\n",
    "        return state, reward\n",
    "        \n",
    "\n",
    "    def step(self, state, act):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            state: a list variable containing x, y integer coordinates. (i.e., [1, 1]).\n",
    "            act: a string variable (i.e., \"UP\"). All feasible values are [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"].\n",
    "        Output args: \n",
    "            next_state: a list variable containing x, y integer coordinates (i.e., [1, 1])\n",
    "            reward: an integer. it can be either 0 or 1.\n",
    "        \"\"\"\n",
    "        \n",
    "        # CODE HERE: implement the stochastic dynamics as described in Q1. \n",
    "        # Please note, we provide you with the deterministic transition function \"take_action\" below.\n",
    "        # Therefore, you only have to implement the logics of the stochasticity.\n",
    "        next_state = None\n",
    "        \n",
    "\n",
    "        # CODE HERE: compute the reward based on the resulting state\n",
    "        reward = None\n",
    "        \n",
    "\n",
    "        # return the current state, reward\n",
    "        return next_state, reward\n",
    "        \n",
    "\n",
    "    \"\"\" DO NOT CHANGE BELOW \"\"\"\n",
    "    def take_action(self, state, act):\n",
    "        \"\"\"\n",
    "        Input args: \n",
    "            state (list): a list variable containing x, y integer coordinates. (i.e., [1, 1]).\n",
    "            act (string): a string variable (i.e., \"UP\"). All feasible values are [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"].\n",
    "        Output args: \n",
    "            next_state (list): a list variable containing x, y integer coordinates (i.e., [1, 1])\n",
    "        \"\"\"\n",
    "        state = np.array(state)\n",
    "        next_state = state + self.action_space[act]\n",
    "        return next_state.tolist() if next_state.tolist() in self.state_space else state.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Here is the plot function you can use to generate the figure. DO NOT CHANGE\"\"\"\n",
    "# PLOT FUNCTION\n",
    "def plot_func(res_list):\n",
    "    # set the figure size\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # plot each trial\n",
    "    for re in res_list:\n",
    "        plt.plot(list(range(len(res_list[0]))), re, linestyle=\"--\", linewidth=1, alpha=0.7)\n",
    "\n",
    "    # plot mean reward\n",
    "    mean_reward = np.array(res_list).mean(axis=0).tolist()\n",
    "    plt.plot(list(range(len(res_list[0]))), mean_reward, linestyle=\"-\", linewidth=2, color=\"k\")\n",
    "\n",
    "    # plot the figure\n",
    "    plt.ylabel(\"Cumulative reward\")\n",
    "    plt.xlabel(\"Time step\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Implement the manual policy\n",
    "\n",
    "Use this to check your whether your implementation of the step function is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # fix the randomness for reproduction\n",
    "    random.seed(1234)\n",
    "    np.random.seed(1234)\n",
    "\n",
    "    # create the environment\n",
    "    env = FourRooms()\n",
    "    state, reward = env.reset()  # always call reset() before interaction\n",
    "    \n",
    "    # manual time step (YOU CAN CHANGE THIS TO ANY TIME STEP YOU WANT)\n",
    "    time_step = 100\n",
    "\n",
    "    # create a loop\n",
    "    for t in range(time_step):\n",
    "        \n",
    "        # CODE HERE: implement your manual agent/policy function that takes in the action from the standard input\n",
    "        action = None\n",
    "        \n",
    "        # CODE HERE: implement the code to interact with the Four Rooms environment above.\n",
    "        # it should takes in the current state and action and returns the next_state and a reward\n",
    "        # Hint: use the step function that you implement.\n",
    "        next_state, reward = None, None\n",
    "        \n",
    "        \"\"\"DO NOT CHANGE BELOW\"\"\"\n",
    "        # print interaction\n",
    "        print(f\"Step = {t}, state = {state}, action = {action}, next state = {next_state}, reward = {reward}\")\n",
    "        \n",
    "        # reset if the agent reaches the goal\n",
    "        if reward == 1:\n",
    "            print(\"Reset the agent to the start state!\")\n",
    "            state, reward = env.reset()\n",
    "        else:\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Implement a random policy\n",
    "\n",
    "We provide the scaffolding code for running and plotting. Please implement a random policy\n",
    "\n",
    "**Please note: you should read the code carefully before implementing to make sure the variable names are aligned.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # fix the randomness for reproduction\n",
    "    random.seed(1234)\n",
    "    np.random.seed(1234)\n",
    "    \n",
    "    # create the environment\n",
    "    env = FourRooms()\n",
    "\n",
    "    # number of the trail (YOU CAN MODIFIED HERE WITH SMALL VALUES FOR DEBUG ONLY)\n",
    "    trial_num = 10   \n",
    "    # length of each trail (YOU CAN MODIFIED HERE WITH SMALL VALUES FOR DEBUG ONLY)\n",
    "    trial_length = int(1e4)\n",
    "    \n",
    "    # save the rewards for plot\n",
    "    rewards_list = []\n",
    "    \n",
    "    # run experiment\n",
    "    for e in tqdm.tqdm(range(trial_num), desc=\"Run trail\", position=0):\n",
    "        \n",
    "        # reset for every trail\n",
    "        reward_per_trial = []\n",
    "        reward_counter = 0\n",
    "    \n",
    "        # reset the environment \n",
    "        state, reward = env.reset()\n",
    "        \n",
    "        # run each trial\n",
    "        for t in tqdm.tqdm(range(trial_length), desc=\"Episode\", position=1, leave=False):\n",
    "            \n",
    "            \n",
    "            # CODE HERE: please implement a random policy to obtain an action.\n",
    "            # it should return a random action from [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"]\n",
    "            action = None\n",
    "                       \n",
    "            # CODE HERE: please implement the code to get the next state and reward\n",
    "            # it should takes in the current state and action\n",
    "            # it should returns the next_state and reward\n",
    "            next_state, reward = None, None\n",
    "            \n",
    "            \n",
    "            \"\"\"DO NOT CHANGE BELOW\"\"\"\n",
    "            # save the reward\n",
    "            reward_counter += reward\n",
    "            reward_per_trial.append(reward_counter)\n",
    "            \n",
    "            # reset\n",
    "            if reward == 1:\n",
    "                state, reward = env.reset()\n",
    "            else:\n",
    "                state = next_state\n",
    "\n",
    "        # save the rewards\n",
    "        rewards_list.append(reward_per_trial)\n",
    "        \n",
    "# PLOT THE RESULTS\n",
    "plot_func(rewards_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Implement better & worse policies against the Random Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # fix the randomness for reproduction\n",
    "    random.seed(1234)\n",
    "    np.random.seed(1234)\n",
    "    \n",
    "    # create the environment\n",
    "    env = FourRooms()\n",
    "\n",
    "    # number of the trail\n",
    "    trial_num = 10   \n",
    "    # length of each trail\n",
    "    trial_length = int(1e4)\n",
    "    \n",
    "    # save the rewards for plot\n",
    "    rewards_list = []\n",
    "    \n",
    "    # run experiment\n",
    "    for e in tqdm.tqdm(range(trial_num), desc=\"Run trail\", position=0):\n",
    "        \n",
    "        # reset for every trail\n",
    "        reward_per_trial = []\n",
    "        reward_counter = 0\n",
    "    \n",
    "        # reset the environment \n",
    "        state, reward = env.reset()\n",
    "        \n",
    "        # run each trial\n",
    "        for t in tqdm.tqdm(range(trial_length), desc=\"Episode\", position=1, leave=False):\n",
    "            \n",
    "            \n",
    "            # CODE HERE: please implement a policy that is worse than the random policy.\n",
    "            # It should takes in the current state and output an action\n",
    "            action = None\n",
    "                       \n",
    "            # CODE HERE: please implement the code to get the next state and reward\n",
    "            next_state, reward = None, None\n",
    "            \n",
    "            \n",
    "            \"\"\"DO NOT CHANGE BELOW\"\"\"\n",
    "            # save the reward\n",
    "            reward_counter += reward\n",
    "            reward_per_trial.append(reward_counter)\n",
    "            \n",
    "            # reset\n",
    "            if reward == 1:\n",
    "                state, reward = env.reset()\n",
    "            else:\n",
    "                state = next_state\n",
    "\n",
    "        # save the rewards\n",
    "        rewards_list.append(reward_per_trial)\n",
    "        \n",
    "# PLOT THE RESULTS\n",
    "plot_func(rewards_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # fix the randomness for reproduction\n",
    "    random.seed(1234)\n",
    "    np.random.seed(1234)\n",
    "    \n",
    "    # create the environment\n",
    "    env = FourRooms()\n",
    "\n",
    "    # number of the trail\n",
    "    trial_num = 10   \n",
    "    # length of each trail\n",
    "    trial_length = int(1e4)\n",
    "    \n",
    "    # save the rewards for plot\n",
    "    rewards_list = []\n",
    "    \n",
    "    # run experiment\n",
    "    for e in tqdm.tqdm(range(trial_num), desc=\"Run trail\", position=0):\n",
    "        \n",
    "        # reset for every trail\n",
    "        reward_per_trial = []\n",
    "        reward_counter = 0\n",
    "    \n",
    "        # reset the environment \n",
    "        state, reward = env.reset()\n",
    "        \n",
    "        # run each trial\n",
    "        for t in tqdm.tqdm(range(trial_length), desc=\"Episode\", position=1, leave=False):\n",
    "            \n",
    "            \n",
    "            # CODE HERE: please implement a policy that is better than the random policy.\n",
    "            # It should takes in the current state and output an action\n",
    "            action = None\n",
    "                       \n",
    "            # CODE HERE: please implement the code to get the next state and reward\n",
    "            next_state, reward = None, None\n",
    "            \n",
    "            \n",
    "            \"\"\"DO NOT CHANGE BELOW\"\"\"\n",
    "            # save the reward\n",
    "            reward_counter += reward\n",
    "            reward_per_trial.append(reward_counter)\n",
    "            \n",
    "            # reset\n",
    "            if reward == 1:\n",
    "                state, reward = env.reset()\n",
    "            else:\n",
    "                state = next_state\n",
    "\n",
    "        # save the rewards\n",
    "        rewards_list.append(reward_per_trial)\n",
    "        \n",
    "# PLOT THE RESULTS\n",
    "plot_func(rewards_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
