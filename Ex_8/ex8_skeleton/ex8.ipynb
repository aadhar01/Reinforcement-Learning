{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Please install the following python libraries\n",
    "- python3: https://www.python.org/\n",
    "- numpy: https://numpy.org/install/\n",
    "- tqdm: https://github.com/tqdm/tqdm#installation\n",
    "- matplotlib: https://matplotlib.org/stable/users/installing/index.html\n",
    "\n",
    "If you encounter the error: \"IProgress not found. Please update jupyter & ipywidgets\"\n",
    "    \n",
    "Please install the ipywidgets as follows:\n",
    "\n",
    "    with pip, do\n",
    "    - pip install ipywidgets\n",
    "    \n",
    "    with conda, do\n",
    "    - conda install -c conda-forge ipywidgets\n",
    "    \n",
    "Restart your notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Deep Neural Networks using Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will use the Pytorch library (https://pytorch.org/) to build and train our deep neural networks. In the deep learning literature, especially in the research community, Pytorch is SUPER popular due to its automatic differentiation and dynamic computational graph (i.e., the graph is automatically generated, which is different from tensorflow where you have to define them beforehand). Briefly spearking, using Pytorch, you only have to build your neural network, define the forward pass, and the loss function. The library will automatically compute the weights and perform the backpropagation for you. For more details about Pytorch, we recommend you check the tutorails on the offical website to learn the basics (https://pytorch.org/tutorials/). \n",
    "\n",
    "Please try to learn the basics as much as you can. If you have any questions, feel free to ask them on Piazza or TA hours. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Please install the Pytorch library on your computer before you run this notebook.\n",
    "\n",
    "The installation instructions can be found here. (https://pytorch.org/get-started/locally/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tqdm\n",
    "import random\n",
    "import gym\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: Nonlinear Function Approximation with Neural Networks\n",
    "\n",
    "We design this question to help you get familiar with the basics in the Pytorch library. Please read the question carefully in the problem set because we will focus more on Pytorch implementation here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the training set\n",
    "\n",
    "First, we create a training set to train the neural network model. Specifically, the dataset contains N training samples. For each sample, it is represented as (x, y), where x is input to the non-linear function \"f(x) = 1 + x^2\" and y is the output value. \n",
    "\n",
    "We provide a scaffolding code below to generate a training set that contains N samples for a particular non-linear function. Please complete the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Function is used to create a dataset that contains \"num\" samples given a particular non-linear function.\n",
    "    In this case, the non-linear function is f(x) = 1 + x ^ 2.\n",
    "\"\"\" \n",
    "def create_dataset(num, start_val, env_val):\n",
    "    \"\"\" Function that generates a dataset \n",
    "        \n",
    "        Args:\n",
    "            num (int): number of samples to generate\n",
    "            start_val (float): the minimal value of x (included)\n",
    "            end_val (float): the maximal value of x (included)\n",
    "    \n",
    "        Returns:\n",
    "            dataset (list): a list consists of (x, y) pairs.\n",
    "    \"\"\"\n",
    "    def nonlinear_function(val):\n",
    "        \"\"\" CODE HERE: non-linear function: 1 + val ^ 2\n",
    "        \"\"\"\n",
    "        return None\n",
    "\n",
    "    \"\"\" CODE HERE: \n",
    "        - create \"num\" even values between start_val and end_val using numpy.linspace\n",
    "        - create the dataset, which is a list of training tuples. e.g. (x, y)\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the neural network using Pytorch\n",
    "\n",
    "Here, we will define a Neural Network using the Pytorch library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Normally, using Pytorch, we will implement the Neural Network as a class.\n",
    "\n",
    "    Basically, we have to do the following steps:\n",
    "        - Inherit from the torch.nn.Module that contains the basics of Neural Networks in Pytorch.\n",
    "        - Define the architecture of the Neural Network in the \"def __init__() function\"\n",
    "            e.g., you can find the functions to create layers with different types under torch.nn. \n",
    "                  for example, we can build one linear layer use torch.nn.Linear() as follows. \n",
    "        - Define how you would like to perform a forward propagation using you neural network in the \n",
    "          \"def forward()\"\"\n",
    "\"\"\"\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, num_hidden_layer, dim_hidden_layer):\n",
    "        \"\"\" Args:\n",
    "                num_hidden_layer (int): number of the hidden layers\n",
    "                dim_hidden_layer (int): dimension for each hidden layer. You can use different dimensions for\n",
    "                                        different layers. But, we use the same dimension for all hidden layers\n",
    "                                        just for simplicity. \n",
    "                                        \n",
    "            In this exercise, you are asked to design a 4-layers (2 hidden layers) fully connected neural network.\n",
    "        \"\"\"\n",
    "        super(NeuralNet, self).__init__()\n",
    "        \n",
    "        # define the input dimension\n",
    "        self.input_dim = 1\n",
    "\n",
    "        # define the hidden dimension\n",
    "        self.hidden_num = num_hidden_layer\n",
    "\n",
    "        # define the number of the hidden layers\n",
    "        self.hidden_dim = dim_hidden_layer\n",
    "\n",
    "        # define the output dimension\n",
    "        self.output_dim = 1\n",
    "\n",
    "        \"\"\" CODE HERE:\n",
    "            Create a fully connected neural network here\n",
    "        \"\"\"\n",
    "        # define the input linear layer here\n",
    "        \n",
    "        # define the activation function after the input layer (use the ReLU as the activation function)\n",
    "        \n",
    "        # define the first hidden layer here\n",
    "        \n",
    "        # define the activation function after the first hidden layer (use the ReLU as the activation function)\n",
    "        \n",
    "        # define the second hidden layer here\n",
    "        \n",
    "        # define the activation function after the second hidden layer (use the ReLU as the activation function)\n",
    "        \n",
    "        # define the output layer here\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Function that defines the forward propagation \n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\" CODE HERE:\n",
    "            Implement each forward propagation using the corresponding layers you defined above.\n",
    "        \"\"\"\n",
    "        # forward x through the input layer\n",
    "       \n",
    "        # apply activation\n",
    "       \n",
    "        # forward x throught the first hidden layer\n",
    "        \n",
    "        # apply activation\n",
    "        \n",
    "        # forward x throught the second hidden layer\n",
    "        \n",
    "        # apply activation\n",
    "        \n",
    "        # forward x throught the output layer\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the training process\n",
    "\n",
    "Here, we provide you the scaffolding code to train the Neural Network. Using Pytorch, you have to learn to perform 3 essential steps:\n",
    "\n",
    "1. Sample a training batch\n",
    "2. Perform a forward propagation. In other words, given the sampled batch data, compute the prediction values using your Neural Network.\n",
    "3. Perform one step backpropagation. The gradients computation and the backpropagation are done automatically by Pytorch, which is the key reason that it is popular. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" We implement a class to train a neural network\n",
    "\"\"\"\n",
    "class NeuralNetTrainer(object):\n",
    "    def __init__(self, dataset, network_model, params):\n",
    "        \"\"\" To train a Neural Network, we need:\n",
    "                - The training set\n",
    "                - The Neural Network model\n",
    "                - A Loss function\n",
    "                - An Optimizer\n",
    "                - other parameters\n",
    "            Thanks to Pytorch, we can use the built-in Loss function and the Optimizer.\n",
    "            \n",
    "            Args:\n",
    "                dataset (list): a list contains all training data\n",
    "                network_model (nn.Module): a Pytorch defined neural network\n",
    "                params (dict): a dictionary stores the training parameters\n",
    "        \"\"\"\n",
    "        # Dataset \n",
    "        self.dataset = dataset\n",
    "\n",
    "        # We can specify the device to train the model: cpu or GPU\n",
    "        self.device = torch.device(params['device'])\n",
    "        # Send the model to the device\n",
    "        self.model = network_model.to(self.device)\n",
    "\n",
    "        \"\"\" CODE HERE:\n",
    "                - Create the ADAM optimizer\n",
    "                - MSE loss \n",
    "        \"\"\"\n",
    "        # Define the Adam optimizer with specified leaarning rate and weight decay in \"params\"\n",
    " \n",
    "        # We use a simple Mean Square Error (MSE) loss.\n",
    "        \n",
    "        # Save the training parameters\n",
    "        self.params = params\n",
    "\n",
    "    def sample_mini_batch(self, dataset, batch_size):\n",
    "        \"\"\" Function is used to sample a subset of the dataset to train the model.\n",
    "            We usually call it \"mini-batch\" data in machine learning, which is widely\n",
    "            used in stochastic gradient descend.\n",
    "            \n",
    "            Args:\n",
    "                dataset (list):  a list contains all training data\n",
    "                batch_size (int): size of the sampled training data.\n",
    "                \n",
    "            Returns:\n",
    "                input_tensor (torch.tensor): A tensor variable with size |B| x |D_x|, where B = batch_size, and D_x is \n",
    "                                             the dimension of x in one sampled data.\n",
    "                output_tensor (torch.tensor): A tensor variable with size |B| x |D_y|, where B = batch_size, and D_y is \n",
    "                                             the dimension of y in one sampled data.\n",
    "        \"\"\"\n",
    "        # We should always shuffle the whole dataset before sampling\n",
    "        np.random.shuffle(dataset)\n",
    "\n",
    "        \"\"\" CODE HERE:\n",
    "                - Sample a batch of training data\n",
    "        \"\"\"\n",
    "        # Sample a batch data from the original dataset\n",
    "\n",
    "        \"\"\" CODE HERE:\n",
    "                - Split the data into x list and y list\n",
    "        \"\"\"\n",
    "        # Split the x and y in the sampled data\n",
    "\n",
    "        # Convert the input and output into tensor.\n",
    "        input_tensor = torch.tensor(input_data, dtype=torch.float32).to(self.device).view(-1, 1)\n",
    "        output_tensor = torch.tensor(output_data, dtype=torch.float32).to(self.device).view(-1, 1)\n",
    "\n",
    "        return input_tensor, output_tensor\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\" Function is used to run the training \n",
    "        \"\"\"\n",
    "        # Define the number of epochs to train the model\n",
    "        ep_bar = tqdm.trange(self.params['epoch_num'], desc=\"epoch bar\")\n",
    "\n",
    "        # Save the training loss for plotting.\n",
    "        loss = torch.tensor([0])\n",
    "        training_loss = []\n",
    "        \n",
    "        # Loop for every training epoch\n",
    "        for ep in ep_bar:\n",
    "            # For every epoch, we update the model with a fixed number (i.e., iteration_num in params)\n",
    "            # of sampled batch data.\n",
    "            for it in range(self.params['iteration_num']):\n",
    "                # Sample a batch data\n",
    "                x_tensor, gt_y_tensor = self.sample_mini_batch(self.dataset, self.params['batch_size'])\n",
    "                \n",
    "                \"\"\" CODE HERE:\n",
    "                        - Perform a forward propagation\n",
    "                \"\"\"\n",
    "                # Forward propagation\n",
    "                \n",
    "                \n",
    "                \"\"\" COER HERE:\n",
    "                        - Compute the loss after the forward propagation\n",
    "                \"\"\"\n",
    "                # Compute the MSE loss value\n",
    "                \n",
    "                # Save the loss for plotting\n",
    "                training_loss.append(loss.item())\n",
    "\n",
    "                \"\"\" CODE HERE:\n",
    "                        - Complete the backpropagation\n",
    "                \"\"\"\n",
    "                # Perform one step back propagation\n",
    "                \n",
    "            # Set the value to tqdm bar\n",
    "            ep_bar.set_description(f\"Loss = {loss.item()}\")\n",
    "        \n",
    "        # Plot the training loss\n",
    "        plt.title(\"MSE loss curve\")\n",
    "        plt.plot(range(len(training_loss)), training_loss)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_learned_function(self, dataset):\n",
    "        \"\"\" Function to plot the learned non-linear function model (blue) v.s. the groud truth (red)\n",
    "        \n",
    "            Args:\n",
    "                dataset (list): a list variable contains all evaluation (x, y) pairs.\n",
    "        \"\"\"\n",
    "        x_tensor, y_tensor = self.sample_mini_batch(dataset, len(dataset))\n",
    "        \n",
    "        \n",
    "        # compute the prediction for all data to evaluate\n",
    "        with torch.no_grad():\n",
    "            pred_y_tensor = self.model(x_tensor)\n",
    "\n",
    "        # convert the data from tensor to list\n",
    "        gt_x_list = x_tensor.cpu().numpy().tolist()\n",
    "        gt_y_list = y_tensor.cpu().numpy().tolist()\n",
    "        pred_y_list = pred_y_tensor.cpu().numpy().tolist()\n",
    "\n",
    "        # plot the results\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_title(\"Preiction (blue) v.s. Ground truth (red)\")\n",
    "        ax.scatter(gt_x_list, gt_y_list, label=\"gt\", color=\"r\")\n",
    "        ax.scatter(gt_x_list, pred_y_list, label=\"pred\", color=\"b\")\n",
    "        ax.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the loss during the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\n",
    "    'epoch_num': 10,\n",
    "    'iteration_num': 2000,\n",
    "    'learning_rate': 1e-3,\n",
    "    'weight_decay': 5e-4,\n",
    "    'batch_size': 32,\n",
    "    'device': \"cpu\"\n",
    "}\n",
    "\n",
    "my_network = NeuralNet(num_hidden_layer=2, dim_hidden_layer=8)\n",
    "\n",
    "train_dataset = create_dataset(500, -10, 10)\n",
    "\n",
    "my_trainer = NeuralNetTrainer(dataset=train_dataset, network_model=my_network, params=train_params)\n",
    "my_trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the approximation function\n",
    "\n",
    "Here, we want to show how accurate the learned non-linear function is. Specifically, we create three datasets that you have to evaluate as follows:\n",
    "\n",
    "1. One training dataset: it contains all the training data you use to train the model above. (500 samples);\n",
    "2. One test dataset: it contains 100 test samples with x > 10;\n",
    "3. One test dataset: it contains 100 test samples with x < -10.\n",
    "\n",
    "Using the three dataset above, visualize the prediction value as well the ground truth value for all samples in one dataset. Use the \"plot_learned_function\" in the \"NeuralNetTrainer\" class above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training dataset to evaluate\n",
    "train_dataset = create_dataset(500, -10, 10)\n",
    "\n",
    "# test set 1: one test dataset contains samples with 10 < x < 30\n",
    "upper_test_dataset = create_dataset(100, 10, 30)\n",
    "\n",
    "# test set 2: one test dataset contains samples with -30 < x < -10\n",
    "lower_test_dataset = create_dataset(100, -30, -10)\n",
    "\n",
    "# Plot your results for all samples in the training set\n",
    "my_trainer.plot_learned_function(train_dataset)\n",
    "\n",
    "# Plot your results for all samples in test set 1\n",
    "my_trainer.plot_learned_function(upper_test_dataset)\n",
    "\n",
    "# Plot your results for all samples in test set 2\n",
    "my_trainer.plot_learned_function(lower_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2: DQN with Four Rooms\n",
    "\n",
    "Here, let's write a DQN agent to resolve the FourRooms problem in this question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Here is the implementation of the FourRooms\n",
    "    Note that, the reward function is changed to be:\n",
    "        - If the agent reaches the goal, it receives 0 and the episode terminates.\n",
    "        - For other time step, the agent receives -1 reward.\n",
    "\"\"\"\n",
    "class FourRooms(object):\n",
    "    def __init__(self):\n",
    "        # We define the grid for the Four Rooms domain\n",
    "        self.grid = np.array([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                              [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]])\n",
    "\n",
    "        # We define the observation space consisting of all empty cells\n",
    "        # Note: We have to flip the coordinates from (row_idx, column_idx) -> (x, y),\n",
    "        # where x = column_idx, y = 10 - row_idx\n",
    "        self.observation_space = np.argwhere(self.grid == 0.0).tolist()  # Fine all empty cells\n",
    "        self.observation_space = self.arr_coords_to_four_room_coords(self.observation_space)\n",
    "\n",
    "        # We define the action space\n",
    "        self.action_space = {'up': np.array([0, 1]),\n",
    "                             'down': np.array([0, -1]),\n",
    "                             'left': np.array([-1, 0]),\n",
    "                             'right': np.array([1, 0])}\n",
    "        self.action_names = ['up', 'down', 'left', 'right']\n",
    "\n",
    "        # We define the start location\n",
    "        self.start_location = [0, 0]\n",
    "\n",
    "        # We define the goal location\n",
    "        self.goal_location = [10, 10]\n",
    "\n",
    "        # We find all wall cells\n",
    "        self.walls = np.argwhere(self.grid == 1.0).tolist()  # find all wall cells\n",
    "        self.walls = self.arr_coords_to_four_room_coords(self.walls)  # convert to Four Rooms coordinates\n",
    "\n",
    "        # This is an episodic task, we define a timeout: maximal time steps = 459\n",
    "        self.max_time_steps = 459\n",
    "\n",
    "        # We define other useful variables\n",
    "        self.agent_location = None  # track the agent's location in one episode.\n",
    "        self.action = None  # track the agent's action\n",
    "        self.t = 0  # track the current time step in one episode\n",
    "\n",
    "    @staticmethod\n",
    "    def arr_coords_to_four_room_coords(arr_coords_list):\n",
    "        \"\"\"\n",
    "        Function converts the array coordinates to the Four Rooms coordinates (i.e, The origin locates at bottom left).\n",
    "        E.g., The coordinates (0, 0) in the numpy array is mapped to (0, 10) in the Four Rooms coordinates.\n",
    "        Args:\n",
    "            arr_coords_list (list): a list variable consists of tuples of locations in the numpy array\n",
    "\n",
    "        Return:\n",
    "            four_room_coords_list (list): a list variable consists of tuples of converted locations in the\n",
    "                                          Four Rooms environment.\n",
    "        \"\"\"\n",
    "        # Note: We have to flip the coordinates from (row_idx, column_idx) -> (x, y),\n",
    "        # where x = column_idx, y = 10 - row_idx\n",
    "        four_room_coords_list = [(column_idx, 10 - row_idx) for (row_idx, column_idx) in arr_coords_list]\n",
    "        return four_room_coords_list\n",
    "\n",
    "    def reset(self):\n",
    "        # We reset the agent's location to the start location\n",
    "        self.agent_location = self.start_location\n",
    "\n",
    "        # We reset the timeout tracker to be 0\n",
    "        self.t = 0\n",
    "\n",
    "        # We set the information\n",
    "        info = {}\n",
    "        return self.agent_location, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            action (string): a string variable (i.e., \"UP\"). All feasible values are [\"up\", \"down\", \"left\", \"right\"].\n",
    "        \"\"\"\n",
    "        # With probability 0.8, the agent takes the correct direction.\n",
    "        # With probability 0.2, the agent takes one of the two perpendicular actions.\n",
    "        # For example, if the correct action is \"LEFT\", then\n",
    "        #     - With probability 0.8, the agent takes action \"LEFT\";\n",
    "        #     - With probability 0.1, the agent takes action \"UP\";\n",
    "        #     - With probability 0.1, the agent takes action \"DOWN\".\n",
    "        if np.random.uniform() < 0.2:\n",
    "            if action == \"left\" or action == \"right\":\n",
    "                action = np.random.choice([\"up\", \"down\"], 1)[0]\n",
    "            else:\n",
    "                action = np.random.choice([\"right\", \"left\"], 1)[0]\n",
    "\n",
    "        # Convert the agent's location to array\n",
    "        loc_arr = np.array(self.agent_location)\n",
    "\n",
    "        # Convert the action name to movement array\n",
    "        act_arr = self.action_space[action]\n",
    "\n",
    "        # Compute the agent's next location\n",
    "        next_agent_location = np.clip(loc_arr + act_arr,\n",
    "                                      a_min=np.array([0, 0]),\n",
    "                                      a_max=np.array([10, 10])).tolist()\n",
    "\n",
    "        # Check if the agent crashes into walls, it stays at the current location.\n",
    "        if tuple(next_agent_location) in self.walls:\n",
    "            next_agent_location = self.agent_location\n",
    "\n",
    "        \"\"\"Note that, the reward function is changed as follows.\n",
    "        \"\"\"\n",
    "        # Compute the reward\n",
    "        reward = 0.0 if next_agent_location == self.goal_location else -1.0\n",
    "\n",
    "        # Check the termination\n",
    "        # If the agent reaches the goal, reward = 0, done = True\n",
    "        # If the time steps reaches the maximal number, reward = -1, done = True.\n",
    "        if reward == 0.0 or self.t == self.max_time_steps:\n",
    "            terminated = True\n",
    "        else:\n",
    "            terminated = False\n",
    "\n",
    "        # Update the agent's location, action and time step trackers\n",
    "        self.agent_location = next_agent_location\n",
    "        self.action = action\n",
    "        self.t += 1\n",
    "\n",
    "        return next_agent_location, reward, terminated, False, {}\n",
    "\n",
    "    def render(self):\n",
    "        # plot the agent and the goal\n",
    "        # empty cell = 0\n",
    "        # wall cell = 1\n",
    "        # agent cell = 2\n",
    "        # goal cell = 3\n",
    "        plot_arr = self.grid.copy()\n",
    "        plot_arr[10 - self.agent_location[1], self.agent_location[0]] = 2\n",
    "        plot_arr[10 - self.goal_location[1], self.goal_location[0]] = 3\n",
    "        plt.clf()\n",
    "        plt.title(f\"state={self.agent_location}, act={self.action}\")\n",
    "        plt.imshow(plot_arr)\n",
    "        plt.show(block=False)\n",
    "        plt.pause(0.1)\n",
    "\n",
    "    @staticmethod\n",
    "    def test():\n",
    "        my_env = FourRooms()\n",
    "        state, _ = my_env.reset()\n",
    "\n",
    "        for _ in range(100):\n",
    "            action = np.random.choice(list(my_env.action_space.keys()), 1)[0]\n",
    "\n",
    "            next_state, reward, done, _, _ = my_env.step(action)\n",
    "            my_env.render()\n",
    "\n",
    "            if done:\n",
    "                state, _ = my_env.reset()\n",
    "            else:\n",
    "                state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Deep Q network\n",
    "\n",
    "Before, we write a DQN agent. Let's define a Deep Q network as we did in Q1. Otherwise, you could also adapt your\n",
    "implementation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customized weight initialization\n",
    "def customized_weights_init(m):\n",
    "    # compute the gain\n",
    "    gain = nn.init.calculate_gain('relu')\n",
    "    # init the convolutional layer\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        # init the params using uniform\n",
    "        nn.init.xavier_uniform_(m.weight, gain=gain)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "    # init the linear layer\n",
    "    if isinstance(m, nn.Linear):\n",
    "        # init the params using uniform\n",
    "        nn.init.xavier_uniform_(m.weight, gain=gain)\n",
    "        nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNet(nn.Module):\n",
    "    def __init__(self, input_dim, num_hidden_layer, dim_hidden_layer, output_dim):\n",
    "        super(DeepQNet, self).__init__()\n",
    "\n",
    "        \"\"\"CODE HERE: construct your Deep neural network\n",
    "        \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"CODE HERE: implement your forward propagation\n",
    "        \"\"\"\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Experience Replay Buffer\n",
    "\n",
    "One main contribution of DQN is proposing to use the replay buffer. Here is the implementation of a simple replay buffer as a list of transitions (i.e., [(s, a, r, s', d), ....]). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    \"\"\" Implement the Replay Buffer as a class, which contains:\n",
    "            - self._data_buffer (list): a list variable to store all transition tuples.\n",
    "            - add: a function to add new transition tuple into the buffer\n",
    "            - sample_batch: a function to sample a batch training data from the Replay Buffer\n",
    "    \"\"\"\n",
    "    def __init__(self, buffer_size):\n",
    "        \"\"\"Args:\n",
    "               buffer_size (int): size of the replay buffer\n",
    "        \"\"\"\n",
    "        # total size of the replay buffer\n",
    "        self.total_size = buffer_size\n",
    "\n",
    "        # create a list to store the transitions\n",
    "        self._data_buffer = []\n",
    "        self._next_idx = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data_buffer)\n",
    "\n",
    "    def add(self, obs, act, reward, next_obs, done):\n",
    "        # create a tuple\n",
    "        trans = (obs, act, reward, next_obs, done)\n",
    "\n",
    "        # interesting implementation\n",
    "        if self._next_idx >= len(self._data_buffer):\n",
    "            self._data_buffer.append(trans)\n",
    "        else:\n",
    "            self._data_buffer[self._next_idx] = trans\n",
    "\n",
    "        # increase the index\n",
    "        self._next_idx = (self._next_idx + 1) % self.total_size\n",
    "\n",
    "    def _encode_sample(self, indices):\n",
    "        \"\"\" Function to fetch the state, action, reward, next state, and done arrays.\n",
    "        \n",
    "            Args:\n",
    "                indices (list): list contains the index of all sampled transition tuples.\n",
    "        \"\"\"\n",
    "        # lists for transitions\n",
    "        obs_list, actions_list, rewards_list, next_obs_list, dones_list = [], [], [], [], []\n",
    "\n",
    "        # collect the data\n",
    "        for idx in indices:\n",
    "            # get the single transition\n",
    "            data = self._data_buffer[idx]\n",
    "            obs, act, reward, next_obs, d = data\n",
    "            # store to the list\n",
    "            obs_list.append(np.array(obs, copy=False))\n",
    "            actions_list.append(np.array(act, copy=False))\n",
    "            rewards_list.append(np.array(reward, copy=False))\n",
    "            next_obs_list.append(np.array(next_obs, copy=False))\n",
    "            dones_list.append(np.array(d, copy=False))\n",
    "        # return the sampled batch data as numpy arrays\n",
    "        return np.array(obs_list), np.array(actions_list), np.array(rewards_list), np.array(next_obs_list), np.array(\n",
    "            dones_list)\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        \"\"\" Args:\n",
    "                batch_size (int): size of the sampled batch data.\n",
    "        \"\"\"\n",
    "        # sample indices with replaced\n",
    "        indices = [np.random.randint(0, len(self._data_buffer)) for _ in range(batch_size)]\n",
    "        return self._encode_sample(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a shedule for epsilon-greedy policy\n",
    "\n",
    "Here, we define a shedule function to return the epsilon for each time step t. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSchedule(object):\n",
    "    \"\"\" This schedule returns the value linearly\"\"\"\n",
    "    def __init__(self, start_value, end_value, duration):\n",
    "        # start value\n",
    "        self._start_value = start_value\n",
    "        # end value\n",
    "        self._end_value = end_value\n",
    "        # time steps that value changes from the start value to the end value\n",
    "        self._duration = duration\n",
    "        # difference between the start value and the end value\n",
    "        self._schedule_amount = end_value - start_value\n",
    "\n",
    "    def get_value(self, time):\n",
    "        # logic: if time > duration, use the end value, else use the scheduled value\n",
    "        \"\"\" CODE HERE: return the epsilon for each time step within the duration.\n",
    "        \"\"\"\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(object):\n",
    "    # initialize the agent\n",
    "    def __init__(self,\n",
    "                 params,\n",
    "                 ):\n",
    "        # save the parameters\n",
    "        self.params = params\n",
    "\n",
    "        # environment parameters\n",
    "        self.action_dim = params['action_dim']\n",
    "        self.obs_dim = params['observation_dim']\n",
    "\n",
    "        # executable actions\n",
    "        self.action_space = params['action_space']\n",
    "\n",
    "        # create value network\n",
    "        self.behavior_policy_net = DeepQNet(input_dim=params['observation_dim'],\n",
    "                                   num_hidden_layer=params['hidden_layer_num'],\n",
    "                                   dim_hidden_layer=params['hidden_layer_dim'],\n",
    "                                   output_dim=params['action_dim'])\n",
    "        # create target network\n",
    "        self.target_policy_net = DeepQNet(input_dim=params['observation_dim'],\n",
    "                                          num_hidden_layer=params['hidden_layer_num'],\n",
    "                                          dim_hidden_layer=params['hidden_layer_dim'],\n",
    "                                          output_dim=params['action_dim'])\n",
    "\n",
    "        # initialize target network with behavior network\n",
    "        self.behavior_policy_net.apply(customized_weights_init)\n",
    "        self.target_policy_net.load_state_dict(self.behavior_policy_net.state_dict())\n",
    "\n",
    "        # send the agent to a specific device: cpu or gpu\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        self.behavior_policy_net.to(self.device)\n",
    "        self.target_policy_net.to(self.device)\n",
    "\n",
    "        # optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.behavior_policy_net.parameters(), lr=params['learning_rate'])\n",
    "\n",
    "    # get action\n",
    "    def get_action(self, obs, eps):\n",
    "        if np.random.random() < eps:  # with probability eps, the agent selects a random action\n",
    "            action = np.random.choice(self.action_space, 1)[0]\n",
    "            return action\n",
    "        else:  # with probability 1 - eps, the agent selects a greedy policy\n",
    "            obs = self._arr_to_tensor(obs).view(1, -1)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.behavior_policy_net(obs)\n",
    "                action = q_values.max(dim=1)[1].item()\n",
    "            return self.action_space[int(action)]\n",
    "\n",
    "    # update behavior policy\n",
    "    def update_behavior_policy(self, batch_data):\n",
    "        # convert batch data to tensor and put them on device\n",
    "        batch_data_tensor = self._batch_to_tensor(batch_data)\n",
    "\n",
    "        # get the transition data\n",
    "        obs_tensor = batch_data_tensor['obs']\n",
    "        actions_tensor = batch_data_tensor['action']\n",
    "        next_obs_tensor = batch_data_tensor['next_obs']\n",
    "        rewards_tensor = batch_data_tensor['reward']\n",
    "        dones_tensor = batch_data_tensor['done']\n",
    "\n",
    "        \"\"\"CODE HERE:\n",
    "                Compute the predicted Q values using the behavior policy network\n",
    "        \"\"\"\n",
    "        # compute the q value estimation using the behavior network\n",
    "\n",
    "        # compute the TD target using the target network\n",
    "\n",
    "        # compute the loss\n",
    "\n",
    "        # minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        td_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return td_loss.item()\n",
    "\n",
    "    # update update target policy\n",
    "    def update_target_policy(self):\n",
    "        # hard update\n",
    "        \"\"\"CODE HERE: \n",
    "                Copy the behavior policy network to the target network\n",
    "        \"\"\"\n",
    "\n",
    "    # auxiliary functions\n",
    "    def _arr_to_tensor(self, arr):\n",
    "        arr = np.array(arr)\n",
    "        arr_tensor = torch.from_numpy(arr).float().to(self.device)\n",
    "        return arr_tensor\n",
    "\n",
    "    def _batch_to_tensor(self, batch_data):\n",
    "        # store the tensor\n",
    "        batch_data_tensor = {'obs': [], 'action': [], 'reward': [], 'next_obs': [], 'done': []}\n",
    "        # get the numpy arrays\n",
    "        obs_arr, action_arr, reward_arr, next_obs_arr, done_arr = batch_data\n",
    "        # convert to tensors\n",
    "        batch_data_tensor['obs'] = torch.tensor(obs_arr, dtype=torch.float32).to(self.device)\n",
    "        batch_data_tensor['action'] = torch.tensor(action_arr).long().view(-1, 1).to(self.device)\n",
    "        batch_data_tensor['reward'] = torch.tensor(reward_arr, dtype=torch.float32).view(-1, 1).to(self.device)\n",
    "        batch_data_tensor['next_obs'] = torch.tensor(next_obs_arr, dtype=torch.float32).to(self.device)\n",
    "        batch_data_tensor['done'] = torch.tensor(done_arr, dtype=torch.float32).view(-1, 1).to(self.device)\n",
    "\n",
    "        return batch_data_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn_agent(env, params):\n",
    "    # create the DQN agent\n",
    "    my_agent = DQNAgent(params)\n",
    "\n",
    "    # create the epsilon-greedy schedule\n",
    "    my_schedule = LinearSchedule(start_value=params['epsilon_start_value'],\n",
    "                                 end_value=params['epsilon_end_value'],\n",
    "                                 duration=params['epsilon_duration'])\n",
    "\n",
    "    # create the replay buffer\n",
    "    replay_buffer = ReplayBuffer(params['replay_buffer_size'])\n",
    "\n",
    "    # training variables\n",
    "    episode_t = 0\n",
    "    rewards = []\n",
    "    train_returns = []\n",
    "    train_loss = []\n",
    "    loss = 0\n",
    "\n",
    "    # reset the environment\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    # start training\n",
    "    pbar = tqdm.trange(params['total_training_time_step'])\n",
    "    last_best_return = 0\n",
    "    for t in pbar:\n",
    "        # scheduled epsilon at time step t\n",
    "        eps_t = my_schedule.get_value(t)\n",
    "        # get one epsilon-greedy action\n",
    "        action = my_agent.get_action(obs, eps_t)\n",
    "\n",
    "        # step in the environment\n",
    "        next_obs, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        # add to the buffer\n",
    "        replay_buffer.add(obs, env.action_names.index(action), reward, next_obs, done)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        # check termination\n",
    "        if done:\n",
    "            # compute the return\n",
    "            G = 0\n",
    "            for r in reversed(rewards):\n",
    "                G = r + params['gamma'] * G\n",
    "\n",
    "            if G > last_best_return:\n",
    "                torch.save(my_agent.behavior_policy_net.state_dict(), f\"./{params['model_name']}\")\n",
    "\n",
    "            # store the return\n",
    "            train_returns.append(G)\n",
    "            episode_idx = len(train_returns)\n",
    "\n",
    "            # print the information\n",
    "            pbar.set_description(\n",
    "                f\"Ep={episode_idx} | \"\n",
    "                f\"G={np.mean(train_returns[-10:]) if train_returns else 0:.2f} | \"\n",
    "                f\"Eps={eps_t}\"\n",
    "            )\n",
    "\n",
    "            # reset the environment\n",
    "            episode_t, rewards = 0, []\n",
    "            obs, _ = env.reset()\n",
    "        else:\n",
    "            # increment\n",
    "            obs = next_obs\n",
    "            episode_t += 1\n",
    "\n",
    "        if t > params['start_training_step']:\n",
    "            # update the behavior model\n",
    "            if not np.mod(t, params['freq_update_behavior_policy']):\n",
    "                \"\"\" CODE HERE:\n",
    "                    Update the behavior policy network\n",
    "                \"\"\"\n",
    "\n",
    "            # update the target model\n",
    "            if not np.mod(t, params['freq_update_target_policy']):\n",
    "                \"\"\" CODE HERE:\n",
    "                    Update the behavior policy network\n",
    "                \"\"\"\n",
    "\n",
    "    # save the results\n",
    "    return train_returns, train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(arr_list, legend_list, color_list, ylabel, fig_title):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        arr_list (list): list of results arrays to plot\n",
    "        legend_list (list): list of legends corresponding to each result array\n",
    "        color_list (list): list of color corresponding to each result array\n",
    "        ylabel (string): label of the Y axis\n",
    "\n",
    "        Note that, make sure the elements in the arr_list, legend_list and color_list are associated with each other correctly.\n",
    "        Do not forget to change the ylabel for different plots.\n",
    "    \"\"\"\n",
    "    # set the figure type\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # PLEASE NOTE: Change the labels for different plots\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xlabel(\"Time Steps\")\n",
    "\n",
    "    # ploth results\n",
    "    h_list = []\n",
    "    for arr, legend, color in zip(arr_list, legend_list, color_list):\n",
    "        # compute the standard error\n",
    "        arr_err = arr.std(axis=0) / np.sqrt(arr.shape[0])\n",
    "        # plot the mean\n",
    "        h, = ax.plot(range(arr.shape[1]), arr.mean(axis=0), color=color, label=legend)\n",
    "        # plot the confidence band\n",
    "        arr_err *= 1.96\n",
    "        ax.fill_between(range(arr.shape[1]), arr.mean(axis=0) - arr_err, arr.mean(axis=0) + arr_err, alpha=0.3,\n",
    "                        color=color)\n",
    "        # save the plot handle\n",
    "        h_list.append(h)\n",
    "\n",
    "    # plot legends\n",
    "    ax.set_title(f\"{fig_title}\")\n",
    "    ax.legend(handles=h_list)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # set the random seed\n",
    "    np.random.seed(1234)\n",
    "    random.seed(1234)\n",
    "    torch.manual_seed(1234)\n",
    "\n",
    "    # create environment\n",
    "    my_env = FourRooms()\n",
    "\n",
    "    # create training parameters\n",
    "    train_parameters = {\n",
    "        'observation_dim': 2,\n",
    "        'action_dim': 4,\n",
    "        'action_space': my_env.action_names,\n",
    "        'hidden_layer_num': 2,\n",
    "        'hidden_layer_dim': 128,\n",
    "        'gamma': 0.99,\n",
    "\n",
    "        'total_training_time_step': 500_000,\n",
    "\n",
    "        'epsilon_start_value': 1.0,\n",
    "        'epsilon_end_value': 0.01,\n",
    "        'epsilon_duration': 250_000,\n",
    "\n",
    "        'replay_buffer_size': 50000,\n",
    "        'start_training_step': 2000,\n",
    "        'freq_update_behavior_policy': 4,\n",
    "        'freq_update_target_policy': 2000,\n",
    "\n",
    "        'batch_size': 32,\n",
    "        'learning_rate': 1e-3,\n",
    "\n",
    "        'model_name': \"four_room.pt\"\n",
    "    }\n",
    "\n",
    "    # create experiment\n",
    "    train_returns, train_loss = train_dqn_agent(my_env, train_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves([np.array([train_returns])], ['dqn'], ['r'], 'discounted return', 'discounted returns wrt episode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves([np.array([train_loss])], ['dqn'], ['r'], 'training loss', 'loss wrt training steps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Q3: DQN with Classic Controls - CartPole\n",
    " \n",
    "In this question, we will exam our implementation in two classic control tasks in gym. Due to the slight difference between the action spaces of the FourRooms and the CartPole/LunarLander, we have to modify the DQNAgent and the training function a little bit. But, you do NOT have to modify the code according to such change. Just complete the code as previously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(object):\n",
    "    # initialize the agent\n",
    "    def __init__(self,\n",
    "                 params,\n",
    "                 ):\n",
    "        # save the parameters\n",
    "        self.params = params\n",
    "\n",
    "        # environment parameters\n",
    "        self.action_dim = params['action_dim']\n",
    "        self.obs_dim = params['observation_dim']\n",
    "\n",
    "        # executable actions\n",
    "        self.action_space = params['action_space']\n",
    "\n",
    "        # create behavior policy network\n",
    "        self.behavior_policy_net = DeepQNet(input_dim=params['observation_dim'],\n",
    "                                            num_hidden_layer=params['hidden_layer_num'],\n",
    "                                            dim_hidden_layer=params['hidden_layer_dim'],\n",
    "                                            output_dim=params['action_dim'])\n",
    "        # create target network\n",
    "        self.target_policy_net = DeepQNet(input_dim=params['observation_dim'],\n",
    "                                          num_hidden_layer=params['hidden_layer_num'],\n",
    "                                          dim_hidden_layer=params['hidden_layer_dim'],\n",
    "                                          output_dim=params['action_dim'])\n",
    "\n",
    "        # initialize target network with behavior network\n",
    "        self.behavior_policy_net.apply(customized_weights_init)\n",
    "        self.target_policy_net.load_state_dict(self.behavior_policy_net.state_dict())\n",
    "\n",
    "        # send the agent to a specific device: cpu or gpu\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        self.behavior_policy_net.to(self.device)\n",
    "        self.target_policy_net.to(self.device)\n",
    "\n",
    "        # optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.behavior_policy_net.parameters(), lr=params['learning_rate'])\n",
    "\n",
    "    # get action\n",
    "    def get_action(self, obs, eps):\n",
    "        if np.random.random() < eps:  # with probability eps, the agent selects a random action\n",
    "            action = self.action_space.sample()\n",
    "        else:  # with probability 1 - eps, the agent selects a greedy policy\n",
    "            obs = self._arr_to_tensor(obs).view(1, -1)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.behavior_policy_net(obs)\n",
    "                action = q_values.max(dim=1)[1].item()\n",
    "        return action\n",
    "\n",
    "    # update behavior policy\n",
    "    def update_behavior_policy(self, batch_data):\n",
    "        # convert batch data to tensor and put them on device\n",
    "        batch_data_tensor = self._batch_to_tensor(batch_data)\n",
    "\n",
    "        # get the transition data\n",
    "        obs_tensor = batch_data_tensor['obs']\n",
    "        actions_tensor = batch_data_tensor['action']\n",
    "        next_obs_tensor = batch_data_tensor['next_obs']\n",
    "        rewards_tensor = batch_data_tensor['reward']\n",
    "        dones_tensor = batch_data_tensor['done']\n",
    "\n",
    "        \"\"\"CODE HERE:\n",
    "                Compute the predicted Q values using the behavior policy network\n",
    "        \"\"\"\n",
    "        # compute the q value estimation using the behavior network\n",
    "\n",
    "        # compute the TD target using the target network\n",
    "\n",
    "        # compute the loss\n",
    "\n",
    "        # minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        td_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return td_loss.item()\n",
    "\n",
    "    # update update target policy\n",
    "    def update_target_policy(self):\n",
    "        # hard update\n",
    "        \"\"\"CODE HERE: \n",
    "                Copy the behavior policy network to the target network\n",
    "        \"\"\"\n",
    "\n",
    "    # load trained model\n",
    "    def load_model(self, model_file):\n",
    "        # load the trained model\n",
    "        self.behavior_policy_net.load_state_dict(torch.load(model_file, map_location=self.device))\n",
    "        self.behavior_policy_net.eval()\n",
    "\n",
    "    # auxiliary functions\n",
    "    def _arr_to_tensor(self, arr):\n",
    "        arr = np.array(arr)\n",
    "        arr_tensor = torch.from_numpy(arr).float().to(self.device)\n",
    "        return arr_tensor\n",
    "\n",
    "    def _batch_to_tensor(self, batch_data):\n",
    "        # store the tensor\n",
    "        batch_data_tensor = {'obs': [], 'action': [], 'reward': [], 'next_obs': [], 'done': []}\n",
    "        # get the numpy arrays\n",
    "        obs_arr, action_arr, reward_arr, next_obs_arr, done_arr = batch_data\n",
    "        # convert to tensors\n",
    "        batch_data_tensor['obs'] = torch.tensor(obs_arr, dtype=torch.float32).to(self.device)\n",
    "        batch_data_tensor['action'] = torch.tensor(action_arr).long().view(-1, 1).to(self.device)\n",
    "        batch_data_tensor['reward'] = torch.tensor(reward_arr, dtype=torch.float32).view(-1, 1).to(self.device)\n",
    "        batch_data_tensor['next_obs'] = torch.tensor(next_obs_arr, dtype=torch.float32).to(self.device)\n",
    "        batch_data_tensor['done'] = torch.tensor(done_arr, dtype=torch.float32).view(-1, 1).to(self.device)\n",
    "\n",
    "        return batch_data_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn_agent(env, params):\n",
    "    # create the DQN agent\n",
    "    my_agent = DQNAgent(params)\n",
    "\n",
    "    # create the epsilon-greedy schedule\n",
    "    my_schedule = LinearSchedule(start_value=params['epsilon_start_value'],\n",
    "                                 end_value=params['epsilon_end_value'],\n",
    "                                 duration=params['epsilon_duration'])\n",
    "\n",
    "    # create the replay buffer\n",
    "    replay_buffer = ReplayBuffer(params['replay_buffer_size'])\n",
    "\n",
    "    # training variables\n",
    "    episode_t = 0\n",
    "    rewards = []\n",
    "    train_returns = []\n",
    "    train_loss = []\n",
    "    loss = 0\n",
    "\n",
    "    # reset the environment\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    # start training\n",
    "    pbar = tqdm.trange(params['total_training_time_step'])\n",
    "    last_best_return = 0\n",
    "    for t in pbar:\n",
    "        # scheduled epsilon at time step t\n",
    "        eps_t = my_schedule.get_value(t)\n",
    "        # get one epsilon-greedy action\n",
    "        action = my_agent.get_action(obs, eps_t)\n",
    "\n",
    "        # step in the environment\n",
    "        next_obs, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        # add to the buffer\n",
    "        replay_buffer.add(obs, action, reward, next_obs, done)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        # check termination\n",
    "        if done or episode_t == params['max_time_step_per_episode'] - 1:\n",
    "            # compute the return\n",
    "            G = 0\n",
    "            for r in reversed(rewards):\n",
    "                G = r + params['gamma'] * G\n",
    "\n",
    "            if G > last_best_return:\n",
    "                torch.save(my_agent.behavior_policy_net.state_dict(), f\"./{params['model_name']}\")\n",
    "\n",
    "            # store the return\n",
    "            train_returns.append(G)\n",
    "            episode_idx = len(train_returns)\n",
    "\n",
    "            # print the information\n",
    "            pbar.set_description(\n",
    "                f\"Ep={episode_idx} | \"\n",
    "                f\"G={np.mean(train_returns[-10:]) if train_returns else 0:.2f} | \"\n",
    "                f\"Eps={eps_t}\"\n",
    "            )\n",
    "\n",
    "            # reset the environment\n",
    "            episode_t, rewards = 0, []\n",
    "            obs, _ = env.reset()\n",
    "        else:\n",
    "            # increment\n",
    "            obs = next_obs\n",
    "            episode_t += 1\n",
    "\n",
    "        if t > params['start_training_step']:\n",
    "            # update the behavior model\n",
    "            if not np.mod(t, params['freq_update_behavior_policy']):\n",
    "                \"\"\" CODE HERE:\n",
    "                    Update the behavior policy network\n",
    "                \"\"\"\n",
    "\n",
    "            # update the target model\n",
    "            if not np.mod(t, params['freq_update_target_policy']):\n",
    "                \"\"\" CODE HERE:\n",
    "                    Update the behavior policy network\n",
    "                \"\"\"\n",
    "\n",
    "    # save the results\n",
    "    return train_returns, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # set the random seed\n",
    "    np.random.seed(1234)\n",
    "    random.seed(1234)\n",
    "    torch.manual_seed(1234)\n",
    "\n",
    "    # create environment\n",
    "    my_env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "    # create training parameters\n",
    "    train_parameters = {\n",
    "        'observation_dim': 4,\n",
    "        'action_dim': 2,\n",
    "        'action_space': my_env.action_space,\n",
    "        'hidden_layer_num': 2,\n",
    "        'hidden_layer_dim': 128,\n",
    "        'gamma': 0.9995,\n",
    "        \n",
    "        'max_time_step_per_episode': 200,\n",
    "\n",
    "        'total_training_time_step': 1_000_000,\n",
    "\n",
    "        'epsilon_start_value': 1.0,\n",
    "        'epsilon_end_value': 0.01,\n",
    "        'epsilon_duration': 500_000,\n",
    "\n",
    "        'replay_buffer_size': 500_000,\n",
    "        'start_training_step': 2000,\n",
    "        'freq_update_behavior_policy': 4,\n",
    "        'freq_update_target_policy': 2000,\n",
    "\n",
    "        'batch_size': 64,\n",
    "        'learning_rate': 1e-3,\n",
    "\n",
    "        'model_name': \"cartpole_v1.pt\"\n",
    "    }\n",
    "\n",
    "    # create experiment\n",
    "    train_returns, train_loss = train_dqn_agent(my_env, train_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves([np.array([train_returns])], ['dqn'], ['r'], 'discounted return', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves([np.array([train_loss])], ['dqn'], ['r'], 'training loss', 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 DQN with Classic Controls - LunarLander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # set the random seed\n",
    "    np.random.seed(1234)\n",
    "    random.seed(1234)\n",
    "    torch.manual_seed(1234)\n",
    "\n",
    "    # create environment\n",
    "    my_env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "    # create training parameters\n",
    "    train_parameters = {\n",
    "        'observation_dim': 8,\n",
    "        'action_dim': 4,\n",
    "        'action_space': my_env.action_space,\n",
    "        'hidden_layer_num': 2,\n",
    "        'hidden_layer_dim': 128,\n",
    "        'gamma': 0.99,\n",
    "        \n",
    "        'max_time_step_per_episode': 1000,\n",
    "\n",
    "        'total_training_time_step': 500000,\n",
    "\n",
    "        'epsilon_start_value': 1.0,\n",
    "        'epsilon_end_value': 0.01,\n",
    "\n",
    "        'replay_buffer_size': 50000,\n",
    "        'start_training_step': 2000,\n",
    "        'freq_update_behavior_policy': 4,\n",
    "        'freq_update_target_policy': 2000,\n",
    "\n",
    "        'batch_size': 32,\n",
    "        'learning_rate': 1e-3,\n",
    "\n",
    "        'model_name': \"lunar_lander.pt\"\n",
    "    }\n",
    "\n",
    "    # create experiment\n",
    "    train_returns, train_loss = train_dqn_agent(my_env, train_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4 Extra Credit, DQN with Atari."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Implement your code here '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Implement your code here \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
